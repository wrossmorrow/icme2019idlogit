{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Several Discrete Choice Models\n",
    "\n",
    "- - -\n",
    "\n",
    "W. Ross Morrow ([wrossmorrow@stanford.edu](mailto:wrossmorrow@stanford.edu), [wrossmorrow.com](https://wrossmorrow.com))\n",
    "\n",
    "Research Analytics Consultant, Stanford GSB\n",
    "\n",
    "February 5th, 2019\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "- - - - \n",
    "\n",
    "In this notebook we'll compare several Discrete Choice Models in a simple situation: single-feature binary choice data. Specifically, the data will be composed of a number of observations, $N$, a number of individuals, $I$, an individual index per observations $i_n$, and a \"yes/no\" dummy $y_n \\in \\{\\pm 1\\}$ standing in for random (but structured) choices. We can draw those choices with a variety of data generating processes, as discussed below. But the basic setup is that we see some number of repeated trials for different individuals in which they answer \"yes\" or \"no\" to some question, and our broad goal is to model the frequency with which respondents say \"yes\" or \"no\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets do our basic imports and function definitions. We'll discuss the models for data $(N,I,\\mathbf{i},\\mathbf{y})$ below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from multiprocessing import Pool , Queue , Process\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "import string\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import rand , randn , randint , choice\n",
    "\n",
    "import cvxpy as cp\n",
    "import ecos\n",
    "\n",
    "from scipy.sparse import csc_matrix , csr_matrix , coo_matrix , issparse\n",
    "from scipy.optimize import minimize , LinearConstraint , BFGS\n",
    "from scipy.stats import norm as gaussian , normaltest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Colormap\n",
    "%matplotlib notebook\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some wrappers around random number generation that are useful here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def randi(A,N) : return randint(0,high=A,size=N) \n",
    "\n",
    "def rands(N) : return np.sign( 2.0*rand(N) - 1.0 )\n",
    "\n",
    "def randc(C,N,p) : return choice( C , size=N , p=p )\n",
    "\n",
    "def randp(A,N) : \n",
    "    L , R = randi(A,N) , randi(A-1,N)\n",
    "    R[ np.where( R >= L )[0] ] += 1\n",
    "    return L , R\n",
    "\n",
    "def random_string( l ) : \n",
    "    return ''.join( random.choice(string.ascii_letters) for m in range(l) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nice simple way to plot sparsity patterns for not-too-large sparse matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def spy( A , b=None ) : \n",
    "    plt.figure( figsize=(10,3) )\n",
    "    plt.xticks([]) ; plt.yticks([])\n",
    "    if issparse(A) : A = A.toarray()\n",
    "    if b is None : \n",
    "        plt.imshow( A != 0 , cmap='Greys' , interpolation='nearest', aspect='auto' )\n",
    "    else : \n",
    "        X = np.zeros( ( A.shape[0] , A.shape[1]+4 ) )\n",
    "        X[:,:A.shape[1]] = A\n",
    "        X[:,A.shape[1]+3] = b\n",
    "        plt.imshow( X != 0 , cmap='Greys' , interpolation='nearest', aspect='auto' )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "- - -\n",
    "\n",
    "Below we review Logit, Latent Class Logit, Random Coefficient Logit, and `idLogit` models and provide code for this specific (and rather simple) situation. \n",
    "\n",
    "First, however, we define a general class `FittableModel` each model-specific class will be derived from. This class has a timeout-enabled fitting method as well as derivative checking routines for gradients and Hessian-vector products. The timeout-enabled fitting method is optional, but is helpful given that the models we try to fit can have dramatically different fit times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FittableModel( object ) : \n",
    "    \n",
    "    def __init__( self ) : \n",
    "        pass\n",
    "    \n",
    "    def fit( self , p0=None , maxtime=None ) : \n",
    "        \n",
    "        if p0 is None : \n",
    "            try : \n",
    "                p0 = self.draw_initial_condition(  )\n",
    "            except AttributeError as e : \n",
    "                p0 = randn( self.Nvars )\n",
    "        else : \n",
    "            p0 = p0.flatten()\n",
    "            \n",
    "        start = time.time()\n",
    "        \n",
    "        if ( maxtime is None ) or ( maxtime <= 0.0 ) : \n",
    "            \n",
    "            self.soln = self.wrapped_solve( p0=p0 )\n",
    "            self.soln['timeout'] = False\n",
    "            \n",
    "        else : \n",
    "            \n",
    "            q = Queue()\n",
    "            p = Process( target=self.wrapped_solve , kwargs={ 'p0' : p0 , 'queue' : q } )\n",
    "            \n",
    "            p.start()\n",
    "            while time.time() - start < maxtime :\n",
    "                p.join( timeout=1 )\n",
    "                if not p.is_alive() : break\n",
    "            if p.is_alive():\n",
    "                p.terminate()\n",
    "                self.soln = { 'timeout' : True }\n",
    "            else : \n",
    "                self.soln = q.get()\n",
    "                self.soln['timeout'] = False\n",
    "                \n",
    "        self.soln['solvertime'] = time.time() - start\n",
    "        self.soln['x0'] = p0\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def wrapped_solve( self , p0=None , queue=None ) :\n",
    "        try : \n",
    "            self.soln = self.solve( p0=p0 )\n",
    "            self.soln['error'] = False\n",
    "        except Exception as e : \n",
    "            print( \"caught solve exception: \" , e )\n",
    "            self.soln = { 'error' : True , 'message' : e }\n",
    "        if queue is not None : queue.put( self.soln )\n",
    "        return self.soln\n",
    "    \n",
    "    def grad_check( self , p=None , verbose=True ) : \n",
    "\n",
    "        \"\"\"\n",
    "        gradient check of an object that has certain attributes (Nvars, obj, and grad)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if p is None : p = rand( self.Nvars )\n",
    "\n",
    "        f0 , g0 = self.obj( p ) , self.grad( p ) # original objective and gradient\n",
    "\n",
    "        Hs = 10.0**np.arange( -10 , 0 , 1 )[::-1] # perturbation sizes\n",
    "        pP = p.copy() # copy for perturbed betas\n",
    "\n",
    "        df = np.zeros( self.Nvars ) # space for dinite differences\n",
    "        ds = np.zeros( Hs.size ) # differences between gradient and finite differences\n",
    "        \n",
    "        # iterations\n",
    "        for h in range( Hs.size ) : \n",
    "            H = Hs[h] # actual perturbation\n",
    "            for k in range( self.Nvars ) : \n",
    "                pP[k] += H # perturb on coordinate k\n",
    "                fk = self.obj( pP ) # evalute objective at perturbed argument\n",
    "                df[k] = ( fk - f0 ) / H # compute finite difference\n",
    "                pP[k] -= H # perturb on coordinate k\n",
    "            ds[h] = np.max( np.abs( g0 - df ) ) # compute difference\n",
    "\n",
    "        if( verbose ) : \n",
    "            for h in range( Hs.size ) : \n",
    "                print( \"%0.16f , %0.16f\" % ( Hs[h] , ds[h] ) )\n",
    "\n",
    "        return Hs , ds\n",
    "\n",
    "    def hessp_check( self , p=None , v=None , verbose=True ) :  \n",
    "\n",
    "        \"\"\"\n",
    "        hessian product check of an object that has certain attributes (Nvars, grad, and hessp)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if p is None : p = rand( self.Nvars )\n",
    "        if v is None : v = rand( self.Nvars )\n",
    "\n",
    "        g0 , h0 = self.grad( p ) , self.hessp( p , v ) # original gradient\n",
    "\n",
    "        Hs = 10.0**np.arange( -10 , 1 , 1 )[::-1] # perturbation sizes\n",
    "        pP = p.copy() # copy for perturbed betas\n",
    "\n",
    "        ds = np.zeros( Hs.size ) # differences between gradient and finite differences\n",
    "\n",
    "        # iterations\n",
    "        for h in range( Hs.size ) : \n",
    "            H = Hs[h] # actual perturbation\n",
    "            pP = p + H * v # perturb by H in direction v\n",
    "            gP = self.grad( pP ) # evalute objective gradient at perturbed argument\n",
    "            df = ( gP - g0 ) / H\n",
    "            ds[h] = np.max( np.abs( df - h0 ) ) # compute difference\n",
    "\n",
    "        if( verbose ) : \n",
    "            for h in range( Hs.size ) : \n",
    "                print( \"%0.16f , %0.16f\" % ( Hs[h] , ds[h] ) )\n",
    "\n",
    "        return Hs , ds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have a specific way of testing derivative accuracy: Compare finite differences (for gradients or for hessian-vector products) for a _decreasing sequence_ of perturbation sizes. Regardless of the function evaluation accuracy, we should see a \"V\" shape in the associated relative error magnitude between computed derivatives and finite differences. The finite difference approximation should be innaccurate for \"large\" perturbations but get more accurate as the perturbation decreases, that is until floating point errors start to accumulate and reduce the approximation accuracy. \n",
    "\n",
    "This is obviously more computationally intensive that comparing derivatives at a single perturbation size and for large enough problems can be quite burdensome. However single-point approximations give no indication of what a \"reasonable\" error should be, and thus are somewhat useless when we don't know how much error exists in our function and derivative evaluations to begin with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logit is the simplest model, with \"yes\" probability\n",
    "$$\n",
    "    \\frac{e^\\beta}{1+e^\\beta}\n",
    "$$\n",
    "and MLE problem\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\min &\\quad \\frac{1}{N} \\sum_{n=1}^N \\log( 1 + e^{-y_n\\beta} ) \\\\\n",
    "    \\text{w.r.t.} &\\quad \\beta \\in \\mathbb{R}\n",
    "\\end{aligned}\n",
    "$$\n",
    "The derivative of the objective is easily derived as\n",
    "$$\n",
    "    D^\\beta = \\frac{1}{N} \\sum_{n=1}^N (-y_n)\\left( \\frac{ e^{-y_n\\beta} }{ 1 + e^{-y_n\\beta} } \\right)\n",
    "$$\n",
    "Moreover the Hessian is even easily derived as\n",
    "$$\n",
    "    \\frac{1}{N} \\sum_{n=1}^N \\left( \\frac{ e^{-y_n\\beta} }{ 1 + e^{-y_n\\beta} } \\right)\\left( 1 - \\frac{ e^{-y_n\\beta} }{ 1 + e^{-y_n\\beta} } \\right)\n",
    "$$\n",
    "The following class, derived from `FittableModel`, implements the Logit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Logit( FittableModel ) : \n",
    "    \n",
    "    type = \"Logit\"\n",
    "    \n",
    "    def __init__( self , N , y ) : \n",
    "        self.Nvars , self.N , self.y , self.ny = 1 , N , y , - y\n",
    "\n",
    "    def obj( self , p ) :\n",
    "        return np.sum( np.log1p( np.exp( self.ny * p ) ) ) / self.N\n",
    "\n",
    "    def grad( self , p ) :  \n",
    "        eU = np.exp( self.ny * p ) ; PL = np.divide( eU , 1.0 + eU )\n",
    "        return np.array( [ np.sum( PL * self.ny ) / self.N ] )\n",
    "    \n",
    "    def hess( self , p ) :  \n",
    "        eU = np.exp( self.ny * p ) ; PL = eU / ( 1.0 + eU )\n",
    "        return np.array( [ np.sum( PL * ( 1.0 - PL ) ) / self.N ] )\n",
    "    \n",
    "    def solve( self , p0=None ) : \n",
    "        self.soln = minimize( self.obj , p0 , jac=self.grad , \\\n",
    "                              hess=self.hess , method='trust-constr' , \\\n",
    "                           options={ 'maxiter' : 100000 , 'gtol' : 1.0e-6 } )\n",
    "        return self.soln\n",
    "    \n",
    "    def printx( self ) : \n",
    "        if self.soln is None : return \"(no solution)\"\n",
    "        return \"%0.2f\" % self.soln['x'][0]\n",
    "    \n",
    "    def getx( self ) : \n",
    "        if self.soln is None : return None\n",
    "        else : return self.soln['x'][0]\n",
    "    \n",
    "    def kld( self , PT ) : \n",
    "        if self.soln['x'][0] > 0 : \n",
    "            P = 1.0 / ( 1.0 + np.exp( -self.soln['x'][0] ) )\n",
    "        else : \n",
    "            P = np.exp( self.soln['x'][0] ) ; P = P / ( 1.0 + P )\n",
    "        return PT * np.log( PT/P ) + (1.0-PT) * np.log( (1.0-PT)/(1.0-P) )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Class Logit\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we presume that each individual is \"drawn\" from one of $C$ classes each with its own coefficient. Our job is to estimate the class coefficients and the mass function for the classes. The simplest version of this problem is: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\\\\n",
    "    \\min &\\quad - \\frac{1}{N} \\sum_{i=1}^I \\log \\left( \\sum_{c=1}^C \\rho_c e^{L_i(\\beta_c)} \\right)    \n",
    "        \\quad\\quad\\text{where}\\quad \n",
    "        L_i(\\theta) = - \\sum_{ n \\in \\mathcal{O}_i } \\log \\Big( 1 + e^{-y_n\\theta} \\Big) \n",
    "        \\\\\n",
    "    \\text{w.r.t.} &\\quad 0 \\leq \\rho_c \\leq 1 \\;\\; , \\;\\; \\beta_c \\in \\mathbb{R} \n",
    "                        \\;\\; \\text{for all} \\;\\; c = 1,\\dotsc,C \\\\\n",
    "    \\text{s.to} &\\quad \\sum_{c=1}^C \\rho_c = 1 \\\\\n",
    "   \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "The derivatives are\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D_c^\\rho \n",
    "        &= - \\frac{1}{N} \\sum_{i=1}^I \\frac{ e^{L_i(\\beta_c)} }{ \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)} }\n",
    "    \\\\\n",
    "    D_c^\\beta \n",
    "        &= - \\frac{1}{N} \\sum_{i=1}^I \\frac{ \\rho_c e^{L_i(\\beta_c)} L_i^\\prime(\\beta_c) }{ \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)} }\n",
    "    \\quad\\quad\\text{where}\\quad \n",
    "    L_i^\\prime(\\theta) \n",
    "        = - \\sum_{ n \\in \\mathcal{O}_i } (-y_n)\\frac{ e^{-y_n\\theta} }{ 1 + e^{-y_n\\theta} }\n",
    "        = \\sum_{ n \\in \\mathcal{O}_i } y_n\\frac{ e^{-y_n\\theta} }{ 1 + e^{-y_n\\theta} }\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stabilization\n",
    "\n",
    "We have to consider an important stabilization technique. If $\\max_c L_i( \\beta_c ) \\ll 0$, then $e^{ L_i( \\beta_c ) } \\approx 0$ and \n",
    "$$\n",
    "    \\log \\left( \\sum_{c=1}^C \\rho_c e^{ L_i( \\beta_c ) } \\right )\n",
    "$$ \n",
    "will not be computable. Specifically, if \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\\\\n",
    "    \\max_c L_i( \\beta_c ) \\leq -745.15\n",
    "    \\quad\\quad\\text{then}\\quad\\quad\n",
    "    \\texttt{float}( e^{ L_i( \\beta_c ) } ) = 0\n",
    "    \\\\\n",
    "    \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\texttt{float}$ is the floating point representation of the exponential (in double precision). This has to be handled carefully if we want to allow for sufficient exploration of $\\beta_1,\\dotsc,\\beta_C$ for arbitrary data. \n",
    "\n",
    "In particular, note that \n",
    "$$\n",
    "    L_i(\\theta) = - |\\{n:y_n=+1\\}| \\log\\left( 1 + e^{-\\theta} \\right) - |\\{n:y_n=-1\\}| \\log\\left( 1 + e^{\\theta} \\right)\n",
    "$$\n",
    "This is, in fact, a formula we could use to dramatically improve the scalability of the fitting methods we explore, but at the cost of _complete_ loss of generality. What is worth noting here is that as the number of observations increase, the size of both sets goes to infinity; moreover the $\\log$ terms are both positive, having arguments larger than one. Thus, for any finite $\\theta$, there is _some_ number of observations such that $L_i$ is too negative to effectively compute with. It is probably even possible to analyze this situation further, but knowing exactly _when_ we get into trouble isn't helpful. \n",
    "\n",
    "A simple trick sidesteps this potential problem: Let \n",
    "$$\n",
    "L_i^*(\\boldsymbol{\\beta}) = \\max_c L_i( \\beta_c )\n",
    "$$\n",
    "and then\n",
    "$$\n",
    "   -\\frac{1}{N} \\sum_{i=1}^I \\log \\left( \\sum_{c=1}^C \\rho_c e^{ L_i( \\beta_c ) } \\right )\n",
    "        = -\\frac{1}{N} \\sum_{i=1}^I L_i^*(\\boldsymbol{\\beta}) -\\frac{1}{N} \\sum_{i=1}^I  \\log \\left( \\sum_{c=1}^C \\rho_c e^{ L_i( \\beta_c ) - L_i^*(\\boldsymbol{\\beta}) } \\right )\n",
    "$$ \n",
    "Note also that the derivatives can be evaluated with $L_i( \\beta_c ) - L_i^*(\\boldsymbol{\\beta})$: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D_c^\\rho \n",
    "        &= - \\frac{1}{N} \\sum_{i=1}^I \\frac{ e^{L_i(\\beta_c)} }{ \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)} }\n",
    "        = - \\frac{1}{N} \\sum_{i=1}^I \\frac{ e^{L_i^*(\\boldsymbol{\\beta})} e^{L_i(\\beta_c)-L_i^*(\\boldsymbol{\\beta})} }{ e^{L_i^*(\\boldsymbol{\\beta})} \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)-L_i^*(\\boldsymbol{\\beta})} }\n",
    "        = - \\frac{1}{N} \\sum_{i=1}^I \\frac{ e^{L_i(\\beta_c)-L_i^*(\\boldsymbol{\\beta})} }{\\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)-L_i^*(\\boldsymbol{\\beta})} }\n",
    "    \\\\\n",
    "    D_c^\\beta \n",
    "        &= - \\frac{1}{N} \\sum_{i=1}^I \\frac{ \\rho_c e^{L_i(\\beta_c)} L_i^\\prime(\\beta_c) }{ \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)} }\n",
    "        = - \\frac{1}{N} \\sum_{i=1}^I \\frac{ e^{L_i^*(\\boldsymbol{\\beta})} \\rho_c e^{L_i(\\beta_c)-L_i^*(\\boldsymbol{\\beta})} L_i^\\prime(\\beta_c) }{ e^{L_i^*(\\boldsymbol{\\beta})} \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)-L_i^*(\\boldsymbol{\\beta})} }\n",
    "        = - \\frac{1}{N} \\sum_{i=1}^I \\frac{ \\rho_c e^{L_i(\\beta_c)-L_i^*(\\boldsymbol{\\beta})} L_i^\\prime(\\beta_c) }{ \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)-L_i^*(\\boldsymbol{\\beta})} }\n",
    "    \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LatentClassLogit( FittableModel ) : \n",
    "    \n",
    "    type = \"Latent Class Logit\"\n",
    "    \n",
    "    def __init__( self , N , I , i , y , C , ordered=True ) : \n",
    "        \n",
    "        if C <= 1 : \n",
    "            raise ArgumentError( \"Trivial number of classes: should be at least two.\" )\n",
    "        \n",
    "        self.N , self.I , self.i , self.y , self.C = N , I , i , -y , C\n",
    "        self.Nvars , self.Ncons = 2*C , 2*C if ordered else C+1\n",
    "        \n",
    "        # this matrix \"assigns\" observations to individuals, facilitating \n",
    "        # sums over observations within individuals\n",
    "        self.reducer = csr_matrix( (-np.ones(N),(i,np.arange(N))) , shape=(I,N) )\n",
    "        self.yeducer = csr_matrix( (y,(i,np.arange(N))) , shape=(I,N) )\n",
    "        \n",
    "        # constraints: \n",
    "        #  \n",
    "        #     p[0] , p[1] , ... , p[C] >= 0.0\n",
    "        #     p[0] + p[1] + ... + p[C] = 1.0\n",
    "        #     p[0] >= p[1] >= ... >= p[C] if ordered\n",
    "        # \n",
    "        \n",
    "        lo = np.zeros( self.Ncons )\n",
    "        up = np.inf * np.ones( self.Ncons )\n",
    "        lo[C] , up[C] = 1.0 , 1.0\n",
    "        Cmtrx = np.zeros( ( self.Ncons , 2*C ) )\n",
    "        for c in range(self.C) : \n",
    "            Cmtrx[c,c] = 1.0 # p[c] >= 0\n",
    "            Cmtrx[C,c] = 1.0 # sum_c p[c] = 1\n",
    "        if ordered : \n",
    "            for c in range(1,self.C) : \n",
    "                Cmtrx[C+c,c-1] = 1.0\n",
    "                Cmtrx[C+c,c] = - 1.0\n",
    "            \n",
    "        self.cons = LinearConstraint( Cmtrx , lo , up )\n",
    "        \n",
    "        # workspace\n",
    "        self.yp = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.eU = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.ll = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.Li = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.EL = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.PL = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.LP = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.j  = np.zeros((self.I,),dtype=np.float)\n",
    "        self.LM = np.zeros((self.I,),dtype=np.float)\n",
    "        \n",
    "    def basics( self , p ) : \n",
    "        np.outer( self.y , p[self.C:] , out=self.yp )\n",
    "        np.exp( self.yp , out=self.eU )\n",
    "        np.log1p( self.eU , out=self.ll )\n",
    "        self.Li = self.reducer @ self.ll\n",
    "        np.max( self.Li , axis=1 , out=self.LM )\n",
    "        self.Li = self.Li - np.tile( self.LM.reshape((self.I,1)) , (1,self.C) )\n",
    "        np.exp( self.Li , out=self.EL )\n",
    "        self.j = self.EL @ p[:self.C]\n",
    "        \n",
    "    def obj( self , p ) :\n",
    "        self.basics( p )\n",
    "        np.log( self.j , out=self.j )\n",
    "        return - np.sum( self.LM + self.j ) / self.N\n",
    "\n",
    "    def grad( self , p ) :  \n",
    "        self.basics( p ) \n",
    "        np.divide( self.eU , 1.0 + self.eU , out=self.PL )\n",
    "        self.LP = self.yeducer @ self.PL\n",
    "        self.j  = 1.0 / self.j\n",
    "        \n",
    "        # gradient terms\n",
    "        g = np.zeros( self.Nvars )\n",
    "        g[:self.C] = - self.EL.T @ self.j / self.N\n",
    "        g[self.C:] = - p[:self.C] * ( ( self.EL * self.LP ).T @ self.j ) / self.N\n",
    "        \n",
    "        return g\n",
    "    \n",
    "    def draw_initial_condition( self ) : \n",
    "        p0 = randn( self.Nvars ) \n",
    "        p0[:self.C] = np.abs( p0[:self.C] )\n",
    "        p0[:self.C] = p0[:self.C] / p0[:self.C].sum()\n",
    "        return p0\n",
    "    \n",
    "    def solve( self , p0=None ) : \n",
    "        self.soln = minimize( self.obj , p0 , jac=self.grad , hess=BFGS() , \\\n",
    "                            method='trust-constr' , constraints=self.cons , \\\n",
    "                           options={ 'maxiter' : 100000 , 'gtol' : 1.0e-6 } )\n",
    "        return self.soln\n",
    "    \n",
    "    def printx( self ) : \n",
    "        if self.soln is None : return \"(no solution)\"\n",
    "        s = \" ) , ( \".join( [ \"%0.2f , %0.2f\" % ( self.soln['x'][c+self.C] , self.soln['x'][c] ) \\\n",
    "                                    for c in range(self.C) ] )\n",
    "        return \"( %s )\" % s\n",
    "    \n",
    "    def getx( self ) : \n",
    "        if self.soln is None : return None\n",
    "        else : \n",
    "            return self.soln['x'][self.C:]\n",
    "    \n",
    "    def kld( self , PT ) : \n",
    "        PL = np.exp( self.soln['x'][self.C:] ) ; PL = PL / ( 1.0 + PL )\n",
    "        P  = np.sum( PL * self.soln['x'][:self.C] )\n",
    "        return PT * np.log( PT/P ) + (1.0-PT) * np.log( (1.0-PT)/(1.0-P) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Classification\" Latent Class Logit\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we \"classify\" individuals to classes, instead of presume each individual was drawn from a class at random. For this we introduce individual-specific class membership probabilities and solve\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\min &\\quad - \\frac{1}{N} \\sum_{i=1}^I \\log \\left( \\sum_{c=1}^C \\rho_{i,c} e^{L_i(\\beta_c)} \\right) \\\\\n",
    "    \\text{w.r.t.} &\\quad 0 \\leq \\rho_{i,c} \\leq 1 \\;\\; , \\;\\; \\beta_c \\in \\mathbb{R} \n",
    "                        \\;\\; \\text{for all} \\;\\; c = 1,\\dotsc,C \\\\\n",
    "    \\text{s.to} &\\quad \\sum_{c=1}^C \\rho_{i,c} = 1 \\text{ for all } i \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "The derivatives here are simpler, if more numerous: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D_{i,c}^\\rho \n",
    "        &= - \\frac{1}{N} \\frac{ e^{L_i(\\beta_c)} }{ \\sum_{d=1}^C \\rho_{i,d} e^{L_i(\\beta_d)} }\n",
    "        \\\\\n",
    "    D_{c}^\\beta \n",
    "        &= - \\frac{1}{N} \\sum_{i=1}^I \\frac{ \\rho_{i,c} e^{L_i(\\beta_c)} }{ \\sum_{d=1}^C \\rho_{i,d} e^{L_i(\\beta_d)} }\n",
    "            L_i^\\prime(\\beta_c)\n",
    "        \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "This problem is also not always well-determined, as the large number of variables ($(I+1)C$) can exceed the number of observations ($N$). \n",
    "\n",
    "We stabilize in the same way as described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClassClassLogit( FittableModel ) : \n",
    "    \n",
    "    type = \"Class Class Logit\"\n",
    "    \n",
    "    def __init__( self , N , I , i , y , C , ordered=True ) : \n",
    "        \n",
    "        if C <= 1 : \n",
    "            raise ArgumentError( \"Trivial number of classes: should be at least two.\" )\n",
    "        \n",
    "        self.N , self.I , self.i , self.y , self.C = N , I , i , -y , C\n",
    "        self.Nvars , self.IC , self.Ncons = (I+1)*C , I*C , I*(C+1)\n",
    "        \n",
    "        if self.N - self.Nvars <= 0 : \n",
    "            raise ArgumentError( \"Not enough observations for the number of individuals and classes.\" )\n",
    "        \n",
    "        # this matrix \"assigns\" observations to individuals, facilitating \n",
    "        # sums over observations within individuals\n",
    "        self.reducer = csr_matrix( (-np.ones(N),(i,np.arange(N))) , shape=(I,N) )\n",
    "        self.yeducer = csr_matrix( (y,(i,np.arange(N))) , shape=(I,N) )\n",
    "        \n",
    "        # constraints: \n",
    "        #  \n",
    "        #     p[0] , p[1] , ... , p[IC-1] >= 0.0                          IC equations\n",
    "        #     p[i,0] + p[i,1] + ... + p[i,C-1] = 1.0 for all I            I equations\n",
    "        #     p[i,0] - p[i,1] , ... p[i,C] - p[i,C-1] >= 0 if ordered ?   I(C-1) equations\n",
    "        # \n",
    "        \n",
    "        lo , up = np.zeros( self.Ncons ) , np.inf * np.ones( self.Ncons )\n",
    "        lo[I*C:I*(C+1)] , up[I*C:I*(C+1)] = 1.0 , 1.0\n",
    "        \n",
    "        Cnnzs = 2*I*C\n",
    "        Crows , Ccols = np.zeros( Cnnzs , dtype=np.int ) , np.zeros( Cnnzs , dtype=np.int )\n",
    "        Cdata = np.ones( Cnnzs )\n",
    "        \n",
    "        Crows[:I*C] , Ccols[:I*C] = np.arange(I*C) , np.arange(I*C)\n",
    "        Crows[I*C:] , Ccols[I*C:] = I*C + np.repeat( np.arange(I) , C ) , np.arange(I*C)\n",
    "        \n",
    "        Cmtrx = csr_matrix( (Cdata,(Crows,Ccols)) , shape=( self.Ncons , self.Nvars ) )\n",
    "            \n",
    "        self.cons = LinearConstraint( Cmtrx , lo , up )\n",
    "        \n",
    "        # workspace\n",
    "        self.yp = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.eU = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.ll = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.Li = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.EL = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.TL = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.PL = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.LP = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.j  = np.zeros((self.I,),dtype=np.float)\n",
    "        self.LM = np.zeros((self.I,),dtype=np.float)\n",
    "\n",
    "    def basics( self , p ) : \n",
    "        np.outer( self.y , p[self.IC:] , out=self.yp )\n",
    "        np.exp( self.yp , self.eU )\n",
    "        np.log1p( self.eU , out=self.ll )\n",
    "        self.Li = self.reducer @ self.ll\n",
    "        np.max( self.Li , axis=1 , out=self.LM )\n",
    "        self.Li = self.Li - np.tile( self.LM.reshape((self.I,1)) , (1,self.C) )\n",
    "        np.exp( self.Li , out=self.EL )\n",
    "        np.multiply( self.EL , p[:self.IC].reshape((self.I,self.C)) , out=self.TL )\n",
    "        self.j = np.sum( self.TL , axis=1 )\n",
    "        \n",
    "    def obj( self , p ) :\n",
    "        self.basics( p )\n",
    "        np.log( self.j , out=self.j )\n",
    "        return - np.sum( self.LM + self.j ) / self.N\n",
    "\n",
    "    def grad( self , p ) :  \n",
    "        \n",
    "        self.basics( p )\n",
    "        \n",
    "        np.divide( self.eU , 1.0 + self.eU , out=self.PL )\n",
    "        self.LP = self.yeducer @ self.PL\n",
    "        self.j = 1.0 / self.j\n",
    "        \n",
    "        # gradient terms\n",
    "        g = np.zeros( self.Nvars )\n",
    "        g[:self.IC] = - self.EL.flatten() * np.repeat( self.j , self.C ) / self.N\n",
    "        g[self.IC:] = - ( ( self.TL * self.LP ).T @ self.j ) / self.N\n",
    "        \n",
    "        return g\n",
    "    \n",
    "    def draw_initial_condition( self ) : \n",
    "        p0 = randn(self.Nvars)\n",
    "        p0[:self.IC] = np.abs( p0[:self.IC] )\n",
    "        for i in range( self.I ) : \n",
    "            p0[i*self.C:i*self.C+1] = p0[i*self.C:i*self.C+1] / p0[i*self.C:i*self.C+1].sum()\n",
    "        return p0\n",
    "        \n",
    "    def solve( self , p0=None ) : \n",
    "        self.soln = minimize( self.obj , p0 , jac=self.grad , hess=BFGS() , \\\n",
    "                            method='trust-constr' , constraints=self.cons , \\\n",
    "                           options={ 'maxiter' : 100000 , 'gtol' : 1.0e-6 } )\n",
    "        return self.soln\n",
    "    \n",
    "    def printx( self ) : \n",
    "        if self.soln is None : return \"(no solution)\"\n",
    "        rho = self.soln['x'][:self.IC].reshape((self.I,self.C)).mean( axis=0 )\n",
    "        s = \" ) , ( \".join( [ \"%0.2f , %0.2f\" % ( self.soln['x'][self.IC+c] , rho[c] ) for c in range(self.C) ] )\n",
    "        return \"( %s )\" % s\n",
    "    \n",
    "    def getx( self ) : \n",
    "        if self.soln is None : return None\n",
    "        else : \n",
    "            w = self.soln['x'][:self.IC].reshape( (self.I,self.C) ).mean( axis=0 ) # average class likelihoods\n",
    "            return self.soln['x'][self.IC:]\n",
    "    \n",
    "    def kld( self , PT ) : \n",
    "        PL = np.exp( self.soln['x'][self.IC:] ) ; PL = PL / ( 1.0 + PL )\n",
    "        w  = self.soln['x'][:self.IC].reshape( (self.I,self.C) ).mean( axis=0 ) # average class likelihoods\n",
    "        P  = np.sum( PL * w )\n",
    "        return PT * np.log( PT/P ) + (1.0-PT) * np.log( (1.0-PT)/(1.0-P) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Gaussian) Random Coefficients\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random (gaussian) coefficient Logit for this situation is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\min &\\quad -\\frac{1}{N} \\sum_{i=1}^I \\log \\left( \n",
    "                        \\int e^{ L_i( \\mu + \\sigma v ) } \\phi(v) dv\n",
    "                    \\right) \\\\\n",
    "    \\text{w.r.t.} &\\quad \\mu \\in \\mathbb{R} \\; , \\; \\sigma \\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $L_i$ has the same definition as above. Broadly speaking, the derivatives are\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D^\\mu \n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\int e^{ L_i( \\mu + \\sigma v ) } L_i^\\prime( \\mu + \\sigma v ) \\phi(v) dv }\n",
    "                                { \\int e^{ L_i( \\mu + \\sigma v ) } \\phi(v) dv } \n",
    "                    \\\\\n",
    "    D^\\sigma\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\int e^{ L_i( \\mu + \\sigma v ) } L_i^\\prime( \\mu + \\sigma v ) v \\phi(v) dv }\n",
    "                                { \\int e^{ L_i( \\mu + \\sigma v ) } \\phi(v) dv }\n",
    "                    \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "we can approximate these directly, or approximate them by differentiating our approximation to the actual integral. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we approximate the integrals with a sequence of $S$ \"standardized\" samples $v_s$ and associated weights $w_s$ that we hold fix over all individuals: \n",
    "$$\n",
    "    \\int e^{ L_i( \\mu + \\sigma v ) } \\phi(v) dv\n",
    "        \\approx \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) }. \n",
    "$$\n",
    "This makes the negative log likelihood approximation \n",
    "$$\n",
    "    -\\frac{1}{N} \\sum_{i=1}^I \\log\\left( \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } \\right) .\n",
    "$$\n",
    "The associated (identically sampled and weighted) derivative approximations are\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D^\\mu \n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } L_i^\\prime( \\mu + \\sigma v_s ) }\n",
    "                                { \\sum_{s=1}^S  w_s e^{ L_i( \\mu + \\sigma v_s ) }  } \n",
    "                    \\\\\n",
    "    D^\\sigma\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } L_i^\\prime( \\mu + \\sigma v_s ) v_s}\n",
    "                                { \\sum_{s=1}^S w_s  e^{ L_i( \\mu + \\sigma v_s ) } } \n",
    "                    \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "It is important that we either hold such approximations fixed over the course of an estimation attempt or ensure that the approximation is so good that changing approximation error does not interfere with optimizer progress. \n",
    "\n",
    "Given $\\mathbf{v}$ and $\\mathbf{w}$, we can compute the negative log likelihood $f$ and gradient $\\mathbf{g}$ as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & \\\\\n",
    "    &\\quad \\boldsymbol{\\theta} \\leftarrow \\mu + \\sigma \\mathbf{v} \\in \\mathbb{R}^{S} \\\\\n",
    "    &\\quad \\boldsymbol{\\eta} \\leftarrow \\exp\\{ - \\mathbf{y} \\boldsymbol{\\theta}^\\top \\} \\in \\mathbb{R}^{N \\times S} \\\\\n",
    "    &\\quad \\boldsymbol{\\ell} \\leftarrow \\log( 1 + \\boldsymbol{\\eta} ) \\in \\mathbb{R}^{N \\times S} \\\\\n",
    "    &\\quad \\mathbf{L} \\leftarrow - \\mathbf{R}\\boldsymbol{\\ell} \\in \\mathbb{R}^{I \\times S} \\\\\n",
    "    &\\quad \\mathbf{P} \\leftarrow \\boldsymbol{\\eta} \\; / \\; (1+\\boldsymbol{\\eta}) \\in \\mathbb{R}^{N \\times S} \\\\\n",
    "    &\\quad \\mathbf{L}^\\prime \\leftarrow \\mathbf{R}(\\mathrm{diag}(\\mathbf{y})\\mathbf{P}) \\in \\mathbb{I \\times S} \\\\\n",
    "    &\\quad \\mathbf{E}_1 \\leftarrow \\exp\\{ \\mathbf{L} \\} \\in \\mathbb{R}^{I \\times S} \\\\\n",
    "    &\\quad \\mathbf{E}_2 \\leftarrow \\mathbf{E}_1 * \\mathbf{L}^\\prime \\in \\mathbb{R}^{I \\times S} \\\\\n",
    "    &\\quad\\begin{aligned}\n",
    "        &\\texttt{for } i=1,\\dotsc,I \\\\\n",
    "        &\\quad\\quad \\mathbf{E}_3[i,:] \\leftarrow \\mathbf{E}_2[i,:] * \\mathbf{v}[:] \\\\\n",
    "     \\end{aligned} \\quad\\quad\\text{or}\\quad\\quad\n",
    "     \\begin{aligned}\n",
    "        &\\texttt{for } s=1,\\dotsc,S \\\\\n",
    "        &\\quad\\quad \\mathbf{E}_3[:,s] \\leftarrow \\mathbf{v}[s]\\;\\mathbf{E}_2[:,s] \\\\\n",
    "     \\end{aligned}\n",
    "    \\\\\n",
    "    &\\quad\\begin{aligned}\n",
    "        &\\texttt{for } k=1,2,3 \\\\\n",
    "        &\\quad\\quad \\mathbf{j}_k \\leftarrow \\mathbf{E}_k \\mathbf{w} \\in \\mathbb{R}^{I} \\\\\n",
    "     \\end{aligned}\n",
    "     \\\\\n",
    "    &\\quad f \\leftarrow - \\; \\texttt{sum}( \\; \\log( \\; \\mathbf{j}_1 \\; ) \\; ) \\; / \\; N \\\\\n",
    "    &\\quad \\mathbf{g} \\leftarrow \n",
    "        - \\frac{1}{N} \\begin{pmatrix} \n",
    "            \\texttt{sum}( \\; \\mathbf{j}_2 \\; / \\; \\mathbf{j}_1 \\; ) \\\\\n",
    "            \\texttt{sum}( \\; \\mathbf{j}_3 \\; / \\; \\mathbf{j}_1 \\; ) \\\\\n",
    "        \\end{pmatrix}\n",
    "     \\\\\n",
    "     & \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "After defining a class for this approach we review two ways of computing sample points and weights: sample average approximations and Gauss-Legendre quadrature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stabilization\n",
    "\n",
    "We have to again consider our stabilization. If $\\max_s L_i( \\mu + \\sigma v_s ) \\ll 0$, then $e^{ L_i( \\mu + \\sigma v_s ) } \\approx 0$; if the latter holds, the resulting $\\log$ will fail. Specifically, if \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\\\\n",
    "    \\max_s L_i( \\mu + \\sigma v_s ) \\leq -745.15\n",
    "    \\quad\\quad\\text{then}\\quad\\quad\n",
    "    \\texttt{float}( e^{ L_i( \\mu + \\sigma v_s ) } ) = 0\n",
    "    \\\\\n",
    "    \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\texttt{float}$ is the floating point representation of the exponential (in double precision). This has to be handled carefully if we want to allow for sufficient exploration of $\\mu,\\sigma$ for arbitrary data. \n",
    "\n",
    "As above, let \n",
    "$$\n",
    "L_i^*(\\mu,\\sigma) = \\max_s L_i( \\mu + \\sigma v_s )\n",
    "$$\n",
    "and note that\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    -\\frac{1}{N} \\sum_{i=1}^I \\log\\left( \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } \\right) \n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \\log\\left( e^{L_i^*(\\mu,\\sigma)} \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } \\right) \\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I L_i^*(\\mu,\\sigma) - \\frac{1}{N} \\sum_{i=1}^I \\log\\left( \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "Here at least one of the exponents, for any $i$, is zero by definition and thus its corresponding exponentiated value is one. Thus the sum is at least as large as the smallest weight, which is nonzero if we carefully construct the weights. \n",
    "\n",
    "If we want, we can absorb the weights too: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    -\\frac{1}{N} \\sum_{i=1}^I \\log\\left( \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } \\right) \n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \\log\\left( \\sum_{s=1}^S e^{ W_{i,s}( \\mu + \\sigma v_s ) } \\right) \n",
    "            &&\\quad W_{i,s}(\\theta) = L_i(\\theta) + \\log w_s\\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \\log\\left( e^{W_i^*(\\mu,\\sigma)} \\sum_{s=1}^S e^{ W_{i,s}( \\mu + \\sigma v_s ) - W_i^*(\\mu,\\sigma) } \\right) \n",
    "            &&\\quad W_i^*(\\mu,\\sigma) = \\max_s W_{i,s}(\\mu+\\sigma v_s) \\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I W_i^*(\\mu,\\sigma) - \\frac{1}{N} \\sum_{i=1}^I \\log\\left( \\sum_{s=1}^S e^{ W_{i,s}( \\mu + \\sigma v_s ) - W_i^*(\\mu,\\sigma) } \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "This form would ensure that the $\\log$ is of a term that is always greater than one. \n",
    "\n",
    "Presuming the first form, the associated derivative approximations can be evaluated the same way with shifted values for $L_i(\\mu+\\sigma v_s)$: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D^\\mu \n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } L_i^\\prime( \\mu + \\sigma v_s ) }\n",
    "                                { \\sum_{s=1}^S  w_s e^{ L_i( \\mu + \\sigma v_s ) }  } \n",
    "                    \\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ e^{L_i^*(\\mu,\\sigma)} \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } L_i^\\prime( \\mu + \\sigma v_s ) }\n",
    "                                { e^{L_i^*(\\mu,\\sigma)} \\sum_{s=1}^S  w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) }  } \n",
    "                    \\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } L_i^\\prime( \\mu + \\sigma v_s ) }\n",
    "                                { \\sum_{s=1}^S  w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) }  } \n",
    "                    \\\\\n",
    "    D^\\sigma\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } L_i^\\prime( \\mu + \\sigma v_s ) v_s}\n",
    "                                { \\sum_{s=1}^S w_s  e^{ L_i( \\mu + \\sigma v_s ) } } \n",
    "                    \\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ e^{L_i^*(\\mu,\\sigma)} \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } L_i^\\prime( \\mu + \\sigma v_s ) v_s}\n",
    "                                { e^{L_i^*(\\mu,\\sigma)} \\sum_{s=1}^S w_s  e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } } \n",
    "                    \\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } L_i^\\prime( \\mu + \\sigma v_s ) v_s}\n",
    "                                { \\sum_{s=1}^S w_s  e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } } \n",
    "                    \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RCLogit( FittableModel ) : \n",
    "    \n",
    "    type = \"Random Coefficient Logit\"\n",
    "    \n",
    "    def __init__( self , N , I , i , y ) : \n",
    "        \n",
    "        self.Nvars , self.N , self.I , self.i , self.y = 2 , N , I , i , -y\n",
    "        \n",
    "        # these matrices \"assign\" observations to individuals, facilitating \n",
    "        # sums over observations within individuals. \n",
    "        self.reducer = csr_matrix( (-np.ones(N),(i,np.arange(N))) , shape=(I,N) )\n",
    "        self.yeducer = csr_matrix( (y,(i,np.arange(N))) , shape=(I,N) ) \n",
    "        \n",
    "        # constraints: sigma is non-negative    \n",
    "        self.cons = LinearConstraint( np.array([[0.0,1.0]]) , np.array([0.0]) , np.array([np.inf]) )\n",
    "        \n",
    "        self.S = 0\n",
    "    \n",
    "    def allocate_workspace( self ) : \n",
    "        if self.S > 0 :\n",
    "            self.eU = np.zeros((self.N,self.S),dtype=np.float) # N x S space\n",
    "            self.ll = np.zeros((self.N,self.S),dtype=np.float) # N x S space\n",
    "            self.Li = np.zeros((self.I,self.S),dtype=np.float) # I x S space\n",
    "            self.PL = np.zeros((self.N,self.S),dtype=np.float) # I x S space\n",
    "            self.LP = np.zeros((self.I,self.S),dtype=np.float) # I x S space\n",
    "            self.E  = np.zeros((self.I,self.S),dtype=np.float) # I x S space\n",
    "            self.j  = np.zeros((self.I,3),dtype=np.float) # I x 3 space\n",
    "            self.LM = np.zeros((self.I,),dtype=np.float) # I space\n",
    "        return\n",
    "    \n",
    "    def basics( self , p ) : \n",
    "        theta = p[0] + p[1] * self.v \n",
    "        np.exp( np.outer( self.y , theta ) , out=self.eU )\n",
    "        np.log1p( self.eU , out=self.ll )\n",
    "        self.Li = self.reducer @ self.ll\n",
    "        \n",
    "        # stabilization\n",
    "        self.LM = np.max( self.Li , axis=1 ) # get largest Li[i,:] -> LM[i]\n",
    "        self.Li = self.Li - np.tile( self.LM.reshape((self.I,1)) , (1,self.S) ) # subtract max from each Li\n",
    "        np.exp( self.Li , self.E ) # E[i,:] <- exp{ Li[i,:] - LM[i] }\n",
    "        self.j[:,0] = self.E @ self.w # each term is at least as large as the minimum weight\n",
    "        \n",
    "    def obj( self , p ) :\n",
    "        self.basics( p ) # this updates things used in both obj and grad\n",
    "        np.log( self.j[:,0] , out=self.j[:,1] ) # log only the shifted terms\n",
    "        return - np.sum( self.LM + self.j[:,1] ) / self.N # have to add in the max terms\n",
    "\n",
    "    def grad( self , p ) :  \n",
    "        \n",
    "        self.basics( p ) # this updates things used in both obj and grad\n",
    "        \n",
    "        np.divide( self.eU , 1.0 + self.eU , out=self.PL )\n",
    "        self.LP = self.yeducer @ self.PL # checked, this is doing the intended sub-sums\n",
    "        np.multiply( self.E , self.LP , out=self.E )\n",
    "        self.j[:,1] = self.E @ self.w\n",
    "        \n",
    "        # if we would guess a loop over i is more efficient, otherwise, loop over s\n",
    "        if self.I <= self.S : \n",
    "            self.E = np.apply_along_axis( lambda e : e * self.v , 1 , self.E )\n",
    "        else : \n",
    "            for s in range(self.S) : self.E[:,s] = self.E[:,s] * self.v[s]\n",
    "        self.j[:,2] = self.E @ self.w\n",
    "        \n",
    "        np.divide( self.j[:,1] , self.j[:,0] , out=self.j[:,1] )\n",
    "        np.divide( self.j[:,2] , self.j[:,0] , out=self.j[:,2] )\n",
    "        \n",
    "        g = np.zeros(2)\n",
    "        g[0] = - np.sum( self.j[:,1] ) / self.N\n",
    "        g[1] = - np.sum( self.j[:,2] ) / self.N\n",
    "        \n",
    "        return g\n",
    "    \n",
    "    def draw_initial_condition( self ) : \n",
    "        p0 = randn( self.Nvars )\n",
    "        p0[1] = np.abs(p0[1])\n",
    "        return p0\n",
    "        \n",
    "    def solve( self , p0=None ) : \n",
    "        self.soln = minimize( self.obj , p0 , jac=self.grad , hess=BFGS() , \\\n",
    "                            method='trust-constr' , constraints=self.cons , \\\n",
    "                           options={ 'maxiter' : 100000 , 'gtol' : 1.0e-6 } )\n",
    "        return self.soln\n",
    "    \n",
    "    def printx( self ) : \n",
    "        if self.soln is None : return \"(no solution)\"\n",
    "        return \"%0.2f ( %0.2f )\" % ( self.soln['x'][0] , self.soln['x'][1] )\n",
    "    \n",
    "    def getx( self ) : \n",
    "        if self.soln is None : return None\n",
    "        else : return self.soln['x'][:2]\n",
    "    \n",
    "    def kld( self , PT ) : \n",
    "        theta = self.soln['x'][0] + self.soln['x'][1] * self.v # mu + sigma v\n",
    "        PL = np.zeros( self.S )\n",
    "        ind = np.where( theta > 0.0 )[0]\n",
    "        PL[ind] = 1.0 / ( 1.0 + np.exp(-theta[ind]) )\n",
    "        ind = np.where( theta <= 0.0 )[0]\n",
    "        PL[ind] = np.exp(theta[ind]) ; PL[ind] = PL[ind] / ( 1.0 + PL[ind] )\n",
    "        P = np.sum( PL * self.w )\n",
    "        return PT * np.log( PT/P ) + (1.0-PT) * np.log( (1.0-PT)/(1.0-P) )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Average Approximations\n",
    "\n",
    "The simplest integral approximation comes from just sample averaging: where we draw $S$ samples $v_s$ from a standard gaussian distribution and use weights $w_s = 1/S$ or maybe even $w_s = \\phi(v_s)$. This is easy, but is also probably the least efficient approximation we can use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GRCLogitSAA( RCLogit ) : \n",
    "        \n",
    "    type = \"Gaussian RC Logit (SAA)\"\n",
    "    \n",
    "    def __init__( self , N , I , i , y , samples=1000 , weighted=False ) : \n",
    "        \n",
    "        # initialize the superclass\n",
    "        RCLogit.__init__( self , N , I , i , y )\n",
    "        \n",
    "        # if we change parameters from now on, redo\n",
    "        self.do_update_data = False\n",
    "        \n",
    "        # should we PDF-weight the samples? \n",
    "        self.weighted = True if weighted else False # \"truthy\" filter for this param\n",
    "        \n",
    "        # get samples\n",
    "        self.resample( samples=samples )\n",
    "        \n",
    "        # allocate workspace\n",
    "        self.allocate_workspace()\n",
    "        \n",
    "        # if we change parameters from now on, redo\n",
    "        self.do_update_data = True\n",
    "        \n",
    "    def resample( self , samples=1000 ) : \n",
    "        \n",
    "        # define basic sampling data: S random normal samples\n",
    "        self.S = samples\n",
    "        self.v = randn( self.S )\n",
    "        \n",
    "        # \"weight\" vector for summing/weighting samples\n",
    "        if self.weighted : \n",
    "            self.w = gaussian.pdf( self.v ) ; self.w = self.w / self.w.sum()\n",
    "        else : \n",
    "            self.w = np.ones( self.S ) / self.S\n",
    "            \n",
    "        if self.do_update_data :\n",
    "            self.allocate_workspace()\n",
    "            \n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss-Legendre Quadrature \n",
    "\n",
    "Divide the real line into $Q+2$ intervals\n",
    "$$\n",
    "    (\\infty,p_1] \\; , \\; (p_1,p_2] \\; , \\; \\dotsc \\; , \\; (p_Q,p_{Q+1}] \\; , \\; (p_{Q+1},\\infty)\n",
    "$$\n",
    "using $Q+1$ points \n",
    "$$\n",
    "    p_1 \\; < \\; p_2 \\; < \\; \\dotsb \\; < \\; p_{Q+1}\n",
    "$$\n",
    "and take \n",
    "$$\n",
    "    \\int e^{ L_i( \\mu + \\sigma v ) }\\phi(v)dv\n",
    "        \\;\\; \\approx \\;\\; \\sum_{q=1}^Q \\int_{p_q}^{p_{q+1}} e^{ L_i( \\mu + \\sigma v ) }\\phi(v)dv\n",
    "$$\n",
    "presuming $p_{1},p_{Q+1}$ are sufficiently large in magnitude so that\n",
    "$$\n",
    "    \\int_{-\\infty}^{p_{1}} e^{ L_i( \\mu + \\sigma v ) }\\phi(v)dv\n",
    "        \\;\\; , \\;\\; \\int_{p_{Q+1}}^{\\infty} e^{ L_i( \\mu + \\sigma v ) }\\phi(v)dv\n",
    "        \\;\\; \\approx \\;\\; 0. \n",
    "$$\n",
    "This allows us to use standard [Gauss-Legendre](https://en.wikipedia.org/wiki/Gaussian_quadrature#Gauss%E2%80%93Legendre_quadrature) approximations\n",
    "$$\n",
    "    \\int_{p_q}^{p_{q+1}} e^{ L_i( \\mu + \\sigma v ) }\\phi(v)dv\n",
    "        \\;\\; \\approx \\;\\; \n",
    "            \\triangle_s \\sum_{k=1}^K \\alpha_k e^{ L_i( \\mu + \\sigma v_{q,k} ) } \\phi(v_{q,k})\n",
    "           \\quad\\quad\n",
    "           v_{q,k} = \\triangle_q \\xi_k + \\Gamma_q\n",
    "$$\n",
    "for some $K$-point integration rule with nodes $\\xi_k \\in [-1,1]$ and weights $\\alpha_k$, where\n",
    "$$\n",
    "    \\triangle_q = \\frac{p_{q+1}-p_{q}}{2}\n",
    "    \\quad\\quad\\text{and}\\quad\\quad\n",
    "    \\Gamma_q = \\frac{p_{q}+p_{q+1}}{2}\n",
    "$$\n",
    "Hence\n",
    "$$\n",
    "       \\int e^{ L_i( \\mu + \\sigma v ) }\\phi(v)dv\n",
    "           \\approx \\sum_{q=1}^Q \\triangle_q \\sum_{k=1}^K \\alpha_k e^{ L_i( \\mu + \\sigma v_{q,k} ) }\n",
    "                    \\phi(v_{q,k})\n",
    "            = \\sum_{s=1}^{QK} w_s e^{ L_i( \\mu + \\sigma v_s ) } \n",
    "$$\n",
    "letting $w_s \\sim \\triangle_q \\alpha_k \\phi(v_s)$ (for an appropriate $s \\to (q,k)$ index transformation). In this way we can absorb these quadrature-specific terms into the weights corresponding to specific samples. \n",
    "\n",
    "To compute, set\n",
    "$$\n",
    "    S = QK\n",
    "    \\quad\\quad\n",
    "    \\mathbf{V} = \\boldsymbol{\\triangle} \\boldsymbol{\\xi}^\\top + \\boldsymbol{\\Gamma}\\mathbf{1}^\\top\n",
    "    \\quad\\quad\n",
    "    \\mathbf{v} = \\mathrm{vec}( \\mathbf{V} )\n",
    "    \\quad\\quad\n",
    "    \\mathbf{w} = \\mathrm{vec}( \\; \\boldsymbol{\\triangle} \\boldsymbol{\\alpha}^\\top * \\phi(\\mathbf{V}) \\; )\n",
    "$$\n",
    "and apply the generic weighted sample formulation above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GRCLogitGLQ( RCLogit ) : \n",
    "        \n",
    "    type = \"Gaussian RC Logit (GLQ)\"\n",
    "    \n",
    "    def __init__( self , N , I , i , y , quad_order=3 , partition=None ) : \n",
    "    \n",
    "        # initialize the superclass\n",
    "        RCLogit.__init__( self , N , I , i , y )\n",
    "        \n",
    "        # don't update in the routines below\n",
    "        self.do_update_data = False\n",
    "        \n",
    "        # define basic quadrature data\n",
    "        self.quadorder( K=quad_order )\n",
    "        \n",
    "        # define the basic partition data\n",
    "        if partition is None : \n",
    "            self.partition()\n",
    "        else : \n",
    "            if 'min' not in partition : partition['min'] = -3.0\n",
    "            if 'max' not in partition : partition['max'] =  4.0\n",
    "            if 'num' not in partition : partition['num'] =  10\n",
    "            self.partition( pmin=partition['min'] , pmax=partition['max'] , pnum=partition['num'] )\n",
    "        \n",
    "        # define the nodes and weights\n",
    "        self.define_nodes_and_weights()\n",
    "        \n",
    "        # allocate workspace\n",
    "        self.allocate_workspace()\n",
    "        \n",
    "        # if we change parameters from now on, redo\n",
    "        self.do_update_data = True\n",
    "        \n",
    "    def plot_quadrature( self ) :\n",
    "        plt.figure( )\n",
    "        for p in self.qp : plt.plot( [p,p] , [0,1] , '--b' )\n",
    "        plt.plot( self.v , 0.0 * np.ones( self.S ) , '.k' )\n",
    "        plt.plot( self.v , gaussian.pdf( self.v ) , '-k' )\n",
    "        return\n",
    "\n",
    "    def quadorder( self , K=3 ) : \n",
    "        \n",
    "        if K < 1 or K > 5 : \n",
    "            raise ArgumentError( \"only handling quadrature for K in {1,2,3,4,5}\" )\n",
    "            \n",
    "        self.K = K\n",
    "        if K == 1 : \n",
    "            self.xi = np.array( [ 0.0 ] )\n",
    "            self.qw = np.array( [ 2.0 ] )\n",
    "        elif K == 2 :\n",
    "            self.xi = np.array( [ -0.57735 , 0.57735 ] )\n",
    "            self.qw = np.array( [  1.00000 , 1.00000 ] )\n",
    "        elif K == 3 :\n",
    "            self.xi = np.array( [ -0.774597 , 0.000000 , 0.774597 ] )\n",
    "            self.qw = np.array( [  0.555556 , 0.888889 , 0.555556 ] )\n",
    "        elif K == 4 :\n",
    "            self.xi = np.array( [ -0.861136 , -0.339981 , 0.339981 , 0.861136 ] )\n",
    "            self.qw = np.array( [  0.347855 ,  0.652145 , 0.652145 , 0.347855 ] )\n",
    "        else :\n",
    "            self.xi = np.array( [ -0.906180 , -0.538469 , 0.000000 , 0.538469 , 0.906180 ] )\n",
    "            self.qw = np.array( [  0.236927 ,  0.478629 , 0.568889 , 0.478629 , 0.236927 ] )\n",
    "            \n",
    "        if self.do_update_data : \n",
    "            self.define_nodes_and_weights()\n",
    "            self.allocate_workspace()\n",
    "            \n",
    "        return\n",
    "        \n",
    "    def partition( self , pmin=-3.0 , pmax=4.0 , pnum=10 ) : \n",
    "        pdel = ( pmax - pmin ) / pnum\n",
    "        p = np.exp( 0.5 * np.arange( pmin , pmax + 0.5*pdel , pdel ) )\n",
    "        self.qp = np.concatenate( ( -p[::-1] , [0] , p )  )\n",
    "        self.Q = len( self.qp ) - 1\n",
    "        self.delta = ( self.qp[1:] - self.qp[:-1] ) / 2.0 # S-vector, approximation interval radii\n",
    "        self.gamma = ( self.qp[1:] + self.qp[:-1] ) / 2.0 # S-vector, approximation interval midpoints \n",
    "        if self.do_update_data : \n",
    "            self.define_nodes_and_weights()\n",
    "            self.allocate_workspace()\n",
    "        return\n",
    "    \n",
    "    def define_nodes_and_weights( self ) : \n",
    "        self.S = self.Q * self.K\n",
    "        V = np.outer( self.delta , self.xi ) + np.tile( self.gamma.reshape((self.Q,1)) , (1,self.K) )\n",
    "        self.v = V.copy().flatten()\n",
    "        self.w = ( np.outer( self.delta , self.qw ) * gaussian.pdf( V ) ).flatten()\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## idLogit\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `idLogit` for this situation is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\max &\\quad \\frac{1}{N} \\sum_{i=1}^I \\log( 1 + e^{-y_n(\\beta+\\delta_{i_n})} )\n",
    "                    + \\frac{\\Lambda_1}{N} || \\boldsymbol{\\delta} ||_1 \n",
    "                    + \\frac{\\Lambda_2}{2N} || \\boldsymbol{\\delta} ||_2^2 \\\\\n",
    "    \\text{w.r.t.} &\\quad \\beta , \\delta_1, \\dotsc , \\delta_I \\\\\n",
    "    \\text{s.to} &\\quad \\delta_1 + \\dotsb + \\delta_I = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "or, in smooth NLP form, \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\max &\\quad \\frac{1}{N} \\sum_{n=1}^N \\log( 1 + e^{-y_n(\\beta+\\delta_{i_n})} )\n",
    "                    + \\frac{\\Lambda_1}{N} \\sum_{i=1}^I s_i\n",
    "                    + \\frac{\\Lambda_2}{2N} || \\boldsymbol{\\delta} ||_2^2 \\\\\n",
    "    \\text{w.r.t.} &\\quad \\beta , \\boldsymbol{\\delta} , \\mathbf{s} \\\\\n",
    "    \\text{s.to} &\\quad \\delta_1 + \\dotsb + \\delta_I = 0 \\\\\n",
    "        &\\quad \\mathbf{s} - \\boldsymbol{\\delta} \\geq \\mathbf{0} \\;\\; , \\;\\; \n",
    "                \\mathbf{s} + \\boldsymbol{\\delta} \\geq \\mathbf{0}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Writing the log likelihood part of the objective as\n",
    "$$\n",
    "    \\frac{1}{N} \\sum_{n=1}^N \\log( 1 + e^{\\mathbf{z}_n^\\top\\mathbf{x}} )\n",
    "    \\quad\\quad \\mathbf{x} = \\begin{pmatrix} \\beta \\\\ \\boldsymbol{\\delta} \\end{pmatrix}\n",
    "$$\n",
    "the gradient is\n",
    "$$\n",
    "    \\begin{pmatrix}\n",
    "        \\frac{1}{N} \\sum_{n=1}^N P_n(\\mathbf{x}) \\mathbf{z}_n\n",
    "        + \\begin{pmatrix} 0 \\\\ \\frac{\\Lambda_2}{N} \\boldsymbol{\\delta} \\end{pmatrix}\n",
    "        \\\\\n",
    "        \\frac{\\Lambda_1}{N} \\mathbf{1}\n",
    "        \\end{pmatrix}\n",
    "        \\quad\\text{where}\\quad\n",
    "        P_n(\\mathbf{x}) = \\frac{ e^{\\mathbf{z}_n^\\top\\mathbf{x}} }{ 1 + e^{\\mathbf{z}_n^\\top\\mathbf{x}} }\n",
    "$$\n",
    "Here the Hessian is even pretty straightforward: The first $1+I$ components are\n",
    "$$\n",
    "    \\frac{1}{N} \\sum_{n=1}^N P_n(\\mathbf{x})(1-P_n(\\mathbf{x})) \\mathbf{z}_n \\mathbf{z}_n^\\top\n",
    "        + \\frac{\\Lambda_2}{N} \\begin{pmatrix} 0 & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{I} \\end{pmatrix}\n",
    "$$\n",
    "and there are no \"$\\mathbf{s}$\" components. Thus Hessian-vector products are\n",
    "$$\n",
    "    \\mathbf{H}\\begin{pmatrix}u\\\\\\mathbf{v}\\\\\\mathbf{w}\\end{pmatrix}\n",
    "        = \\begin{pmatrix} \n",
    "            \\frac{1}{N} \\sum_{n=1}^N \\left( \\mathbf{z}_n^\\top\n",
    "                \\begin{pmatrix} u \\\\ \\mathbf{v} \\end{pmatrix} \\right) P_n(\\mathbf{x})(1-P_n(\\mathbf{x})) \\mathbf{z}_n \n",
    "                + \\frac{\\Lambda_2}{N} \\begin{pmatrix} 0 \\\\ \\mathbf{v} \\end{pmatrix}\n",
    "              \\\\\n",
    "             \\mathbf{0}  \n",
    "        \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class idLogit( FittableModel ) : \n",
    "        \n",
    "    type = \"idLogit\"\n",
    "    \n",
    "    def __init__( self , N , I , i , y , Lambda1=None , Lambda2=None ) : \n",
    "        \n",
    "        self.N , self.I , self.i , self.y = N , I , i , y\n",
    "        self.Nvars , self.Ncons = 1 + 2*I , 1 + 2*I\n",
    "        self.Ip1 = I + 1\n",
    "        \n",
    "        # map of coefficients to \"-y(b+d)\" terms\n",
    "        Znnzs = 2*N\n",
    "        Zdata = np.zeros( Znnzs )\n",
    "        Zrows , Zcols = np.zeros( Znnzs , dtype=np.int ) , np.zeros( Znnzs , dtype=np.int )\n",
    "        \n",
    "        Zdata[:N] , Zdata[N:] = -y , -y\n",
    "        Zrows[:N] , Zrows[N:] = np.arange(N) , np.arange(N) \n",
    "        Zcols[:N] , Zcols[N:] = 0 , 1 + i\n",
    "        \n",
    "        self.Z = csr_matrix( (Zdata,(Zrows,Zcols)) , shape=( N , 1+I ) )\n",
    "        \n",
    "        # spy( self.Z )\n",
    "        \n",
    "        # constraints\n",
    "        \n",
    "        lo , up = np.zeros( self.Ncons ) , np.inf * np.ones( self.Ncons ) ; up[0] = 0.0\n",
    "        \n",
    "        Cnnzs = 5*I\n",
    "        Cdata = np.ones( Cnnzs ) # almost all entries are ones\n",
    "        Crows , Ccols = np.zeros( Cnnzs , dtype=np.int ) , np.zeros( Cnnzs , dtype=np.int )\n",
    "        \n",
    "        Crows[0*I:1*I] , Ccols[0*I:1*I] = 0 , 1 + np.arange(I)\n",
    "        Crows[1*I:2*I] , Ccols[1*I:2*I] , Cdata[1*I:2*I] = 1 + np.arange(I) , 1 + np.arange(I) , -1.0\n",
    "        Crows[2*I:3*I] , Ccols[2*I:3*I] = 1 +     np.arange(I) , 1 + I + np.arange(I)\n",
    "        Crows[3*I:4*I] , Ccols[3*I:4*I] = 1 + I + np.arange(I) , 1 +     np.arange(I)\n",
    "        Crows[4*I:5*I] , Ccols[4*I:5*I] = 1 + I + np.arange(I) , 1 + I + np.arange(I)\n",
    "        \n",
    "        Cmtrx = csr_matrix( (Cdata,(Crows,Ccols)) , shape=( self.Ncons , self.Nvars ) )\n",
    "        \n",
    "        # spy( Cmtrx )\n",
    "            \n",
    "        self.cons = LinearConstraint( Cmtrx , lo , up )\n",
    "        \n",
    "        # initial regularization\n",
    "        L1 = Lambda1 if Lambda1 is not None else self.N\n",
    "        L2 = Lambda2 if Lambda2 is not None else self.N\n",
    "        self.regularize( Lambda1=L1 , Lambda2=L2 )\n",
    "        \n",
    "        self.zp = np.zeros((self.N,),dtype=np.float)\n",
    "        self.eU = np.zeros((self.N,),dtype=np.float)\n",
    "        self.ll = np.zeros((self.N,),dtype=np.float)\n",
    "        self.PL = np.zeros((self.N,),dtype=np.float)\n",
    "\n",
    "    def obj( self , p ) :\n",
    "        np.exp( self.Z @ p[:self.Ip1] , self.eU )\n",
    "        np.log1p( self.eU , self.ll )\n",
    "        return np.sum( self.ll ) / self.N \\\n",
    "                    + self.L1 * np.sum( p[self.Ip1:] ) \\\n",
    "                    + self.L2 * np.sum( p[1:self.Ip1] * p[1:self.Ip1] ) / 2.0\n",
    "        \n",
    "    def grad( self , p ) :  \n",
    "        g = np.zeros( self.Nvars )\n",
    "        np.exp( self.Z @ p[:self.Ip1] , out=self.eU )\n",
    "        np.divide( self.eU , 1.0 + self.eU , out=self.PL )\n",
    "        g[:self.Ip1] = self.Z.T @ self.PL / N\n",
    "        if( self.L2 > 0.0 ) : g[1:self.Ip1] += self.L2 * p[1:self.Ip1]\n",
    "        g[self.Ip1:] = self.L1\n",
    "        return g\n",
    "    \n",
    "    def hessp( self , p , v ) :  \n",
    "        h = np.zeros( self.Nvars )\n",
    "        zv = self.Z @ v[:self.Ip1]\n",
    "        np.exp( self.Z @ p[:self.Ip1] , out=self.eU )\n",
    "        np.divide( self.eU , 1.0 + self.eU , out=self.PL )\n",
    "        self.PL = self.PL * ( 1.0 - self.PL )\n",
    "        h[:self.Ip1] = self.Z.T @ ( zv * self.PL ) / N\n",
    "        if( self.L2 > 0.0 ) : h[1:self.Ip1] += self.L2 * v[1:self.Ip1]\n",
    "        return h\n",
    "    \n",
    "    def regularize( self , Lambda1=None , Lambda2=None ) : \n",
    "        if Lambda1 is not None : self.L1 = Lambda1 / self.N\n",
    "        if Lambda2 is not None : self.L2 = Lambda2 / self.N\n",
    "            \n",
    "    def draw_initial_condition( self ) : \n",
    "        p0 = randn( self.Nvars )\n",
    "        p0[1:] = p0[1:] - p0[1:].mean()\n",
    "        return p0\n",
    "    \n",
    "    def solve( self , p0=None ) : \n",
    "        self.soln = minimize( self.obj , p0 , jac=self.grad , hessp=self.hessp , \\\n",
    "                            method='trust-constr' , constraints=self.cons , \\\n",
    "                           options={ 'maxiter' : 100000 , 'gtol' : 1.0e-6 } )\n",
    "        return self.soln\n",
    "    \n",
    "    def trace( self , p0=None , Lambda1s=None , alpha=None ) : \n",
    "        p0 = self.draw_initial_condition() if p0 is None else p0.flatten()\n",
    "        if Lambda1s is None : Lambda1s = 10.0 ** np.arange(1,10,1)[::-1]\n",
    "        if alpha is None : alpha = 1.0\n",
    "        self.trace = [ None for l in range(Lambda1s.size) ]\n",
    "        for l in range(Lambda1s.size) : \n",
    "            L = Lambda1s[l]\n",
    "            self.regularize( Lambda1=L , Lambda2=alpha*L )\n",
    "            self.trace[l] = self.fit( p0=p0 )\n",
    "            p0 = trace[l].x\n",
    "        return self\n",
    "    \n",
    "    def printx( self ) : \n",
    "        if self.soln is None : return \"(no solution)\"\n",
    "        k2 , p = normaltest( self.soln['x'][1:I+1] )\n",
    "        return \"%0.2f ( %0.2f , %0.4f )\" % ( self.soln['x'][0] , self.soln['x'][1:I+1].std() , p )\n",
    "    \n",
    "    def getx( self ) : \n",
    "        if self.soln is None : return None\n",
    "        else : \n",
    "            k2 , p = normaltest( self.soln['x'][1:I+1] )\n",
    "            return np.array( [ self.soln['x'][0] , self.soln['x'][1:I+1].std() , p ] )\n",
    "    \n",
    "    def kld( self , PT ) : \n",
    "        _ , Ni = np.unique( self.i , return_counts=True )\n",
    "        Wi = Ni / self.N # individual observation weights\n",
    "        B = self.soln['x'][1:I+1] + self.soln['x'][0]\n",
    "        PL = np.zeros( self.I )\n",
    "        ind = np.where( B >  0.0 )[0] ; PL[ind] = 1.0 / ( 1.0 + np.exp(-B[ind]) )\n",
    "        ind = np.where( B <= 0.0 )[0] ; PL[ind] = np.exp(B[ind]) ; PL[ind] = PL[ind] / ( 1.0 + PL[ind] )\n",
    "        P = np.sum( PL * Wi )\n",
    "        return PT * np.log( PT/P ) + (1.0-PT) * np.log( (1.0-PT)/(1.0-P) )\n",
    "    \n",
    "    invphi  = (np.sqrt(5) - 1) / 2 # 1/phi                                                                                                                     \n",
    "    invphi2 = (3 - np.sqrt(5)) / 2 # 1/phi^2\n",
    "    logip2  = np.log( invphi )\n",
    "\n",
    "    def bestLambdas( self , PT , p0=None , alpha=1.0 , tol=1.0e-3 , hot_start=False , trace=True ) : \n",
    "        \n",
    "        p0 = self.draw_initial_condition() if p0 is None else p0.flatten()\n",
    "        \n",
    "        if trace : klds = []\n",
    "        \n",
    "        # choose upper and lower Lambda1 values\n",
    "        Lambda1_b , Lambda1_a = self.N , 0.1 \n",
    "        Lambda1_h = Lambda1_b - Lambda1_a\n",
    "        if Lambda1_h <= tol : \n",
    "            return [[Lambda1_a,alpha*Lambda1_a],[Lambda1_b,alpha*Lambda1_b]]\n",
    "        \n",
    "        # fit at upper Lambda value\n",
    "        self.regularize( Lambda1=Lambda1_b , Lambda2=alpha*Lambda1_b )\n",
    "        self.fit( p0=p0 )\n",
    "        kld_b = self.kld( PT )\n",
    "        x_b = self.soln.x.copy()\n",
    "        if trace : klds.append( [Lambda1_b,kld_b] )\n",
    "        \n",
    "        # fit at lower Lambda value\n",
    "        self.regularize( Lambda1=Lambda1_a , Lambda2=alpha*Lambda1_a )\n",
    "        self.fit( p0=p0 )\n",
    "        kld_a = self.kld( PT )\n",
    "        x_a = self.soln.x.copy()\n",
    "        if trace : klds.append( [Lambda1_a,kld_a] )\n",
    "        \n",
    "        # required steps to achieve tolerance                                                                                                                   \n",
    "        steps = int( np.ceil( np.log( tol / Lambda1_h ) / self.logip2 ) )\n",
    "\n",
    "        # trial point \"c\"\n",
    "        Lambda1_c = Lambda1_a + self.invphi2 * Lambda1_h\n",
    "        self.regularize( Lambda1=Lambda1_c , Lambda2=alpha*Lambda1_c )\n",
    "        self.fit( p0=p0 )\n",
    "        kld_c = self.kld( PT )\n",
    "        x_c = self.soln.x.copy()\n",
    "        if trace : klds.append( [Lambda1_c,kld_c] )\n",
    "        \n",
    "        # trial point \"d\"\n",
    "        Lambda1_d = Lambda1_a + self.invphi  * Lambda1_h\n",
    "        self.regularize( Lambda1=Lambda1_d , Lambda2=alpha*Lambda1_d )\n",
    "        self.fit( p0=p0 )\n",
    "        kld_d = self.kld( PT )\n",
    "        x_d = self.soln.x.copy()\n",
    "        if trace : klds.append( [Lambda1_d,kld_d] )\n",
    "\n",
    "        # steps\n",
    "        for k in range( steps-1 ):\n",
    "            \n",
    "            if kld_c < kld_d :\n",
    "                \n",
    "                # a < c < d < b  -->  ( a' = a ) < ( c' = ? ) < ( d' = c ) < ( b' = b )\n",
    "                \n",
    "                Lambda1_b = Lambda1_d\n",
    "                Lambda1_d = Lambda1_c\n",
    "                kld_d = kld_c\n",
    "                Lambda1_h = self.invphi * Lambda1_h\n",
    "                Lambda1_c = Lambda1_a + self.invphi2 * Lambda1_h\n",
    "                self.regularize( Lambda1=Lambda1_c , Lambda2=alpha*Lambda1_c )\n",
    "                self.fit( p0=( x_c if hot_start else p0  ) )\n",
    "                kld_c = self.kld( PT )\n",
    "                x_c = self.soln.x.copy()\n",
    "                if trace : klds.append( [Lambda1_c,kld_c] )\n",
    "                \n",
    "            else :\n",
    "                \n",
    "                # a < c < d < b  -->  ( a' = c ) < ( c' = d ) < ( d' = ? ) < ( b' = b )\n",
    "                \n",
    "                Lambda1_a = Lambda1_c \n",
    "                Lambda1_c = Lambda1_d\n",
    "                kld_c = kld_d\n",
    "                Lambda1_h = self.invphi * Lambda1_h\n",
    "                Lambda1_d = Lambda1_a + self.invphi  * Lambda1_h\n",
    "                self.regularize( Lambda1=Lambda1_d , Lambda2=alpha*Lambda1_d )\n",
    "                self.fit( p0=( x_b if hot_start else p0  ) )\n",
    "                kld_d = self.kld( PT )\n",
    "                x_d = self.soln.x.copy()\n",
    "                if trace : klds.append( [Lambda1_d,kld_d] )\n",
    "\n",
    "        if kld_c < kld_d:\n",
    "            res = [[Lambda1_a,alpha*Lambda1_a],[Lambda1_d,alpha*Lambda1_d]]\n",
    "        else:\n",
    "            res = [[Lambda1_c,alpha*Lambda1_c],[Lambda1_b,alpha*Lambda1_b]]\n",
    "           \n",
    "        if trace : return res , klds\n",
    "        return res\n",
    "    \n",
    "    def bestLambdas10( self , PT , p0=None , alpha=1.0 , tol=1.0e-3 , hot_start=False , trace=True ) : \n",
    "        \n",
    "        p0 = self.draw_initial_condition() if p0 is None else p0.flatten()\n",
    "        \n",
    "        if trace : klds = []\n",
    "        \n",
    "        # choose upper and lower Lambda1 values\n",
    "        b , a = np.log10(self.N) , np.log10(0.1)\n",
    "        h = b - a\n",
    "        if h <= tol : \n",
    "            return [[10.0**a,alpha*10.0**a],[10.0**b,alpha*10.0**b]]\n",
    "        \n",
    "        # fit at upper Lambda value\n",
    "        Lambda1_b = 10.0**b\n",
    "        self.regularize( Lambda1=Lambda1_b , Lambda2=alpha*Lambda1_b )\n",
    "        self.fit( p0=p0 )\n",
    "        kld_b = self.kld( PT )\n",
    "        x_b = self.soln.x.copy()\n",
    "        if trace : klds.append( [Lambda1_b,kld_b] )\n",
    "        \n",
    "        # fit at lower Lambda value\n",
    "        Lambda1_a = 10.0**b\n",
    "        self.regularize( Lambda1=Lambda1_a , Lambda2=alpha*Lambda1_a )\n",
    "        self.fit( p0=p0 )\n",
    "        kld_a = self.kld( PT )\n",
    "        x_a = self.soln.x.copy()\n",
    "        if trace : klds.append( [Lambda1_a,kld_a] )\n",
    "        \n",
    "        # required steps to achieve tolerance                                                                                                                   \n",
    "        steps = int( np.ceil( np.log( tol / h ) / self.logip2 ) )\n",
    "\n",
    "        # trial point \"c\"\n",
    "        c = a + self.invphi2 * h\n",
    "        Lambda1_c = 10.0**c\n",
    "        self.regularize( Lambda1=Lambda1_c , Lambda2=alpha*Lambda1_c )\n",
    "        self.fit( p0=p0 )\n",
    "        kld_c = self.kld( PT )\n",
    "        x_c = self.soln.x.copy()\n",
    "        if trace : klds.append( [Lambda1_c,kld_c] )\n",
    "        \n",
    "        # trial point \"d\"\n",
    "        d = a + self.invphi  * h\n",
    "        Lambda1_d = 10.0**d\n",
    "        self.regularize( Lambda1=Lambda1_d , Lambda2=alpha*Lambda1_d )\n",
    "        self.fit( p0=p0 )\n",
    "        kld_d = self.kld( PT )\n",
    "        x_d = self.soln.x.copy()\n",
    "        if trace : klds.append( [Lambda1_d,kld_d] )\n",
    "\n",
    "        # steps\n",
    "        for k in range( steps-1 ):\n",
    "            \n",
    "            if kld_c < kld_d :\n",
    "                \n",
    "                # a < c < d < b  -->  ( a' = a ) < ( c' = ? ) < ( d' = c ) < ( b' = b )\n",
    "                \n",
    "                Lambda1_b = Lambda1_d\n",
    "                Lambda1_d = Lambda1_c\n",
    "                kld_d = kld_c\n",
    "                h = self.invphi * h\n",
    "                c = a + self.invphi2 * h\n",
    "                Lambda1_c = 10.0**c\n",
    "                self.regularize( Lambda1=Lambda1_c , Lambda2=alpha*Lambda1_c )\n",
    "                self.fit( p0=( x_c if hot_start else p0  ) )\n",
    "                kld_c = self.kld( PT )\n",
    "                x_c = self.soln.x.copy()\n",
    "                if trace : klds.append( [Lambda1_c,kld_c] )\n",
    "                \n",
    "            else :\n",
    "                \n",
    "                # a < c < d < b  -->  ( a' = c ) < ( c' = d ) < ( d' = ? ) < ( b' = b )\n",
    "                \n",
    "                Lambda1_a = Lambda1_c \n",
    "                Lambda1_c = Lambda1_d\n",
    "                kld_c = kld_d\n",
    "                h = self.invphi * h\n",
    "                d = a + self.invphi  * h\n",
    "                Lambda1_d = 10.0**d\n",
    "                self.regularize( Lambda1=Lambda1_d , Lambda2=alpha*Lambda1_d )\n",
    "                self.fit( p0=( x_b if hot_start else p0  ) )\n",
    "                kld_d = self.kld( PT )\n",
    "                x_d = self.soln.x.copy()\n",
    "                if trace : klds.append( [Lambda1_d,kld_d] )\n",
    "\n",
    "        if kld_c < kld_d:\n",
    "            res = [[Lambda1_a,alpha*Lambda1_a],[Lambda1_d,alpha*Lambda1_d]]\n",
    "        else:\n",
    "            res = [[Lambda1_c,alpha*Lambda1_c],[Lambda1_b,alpha*Lambda1_b]]\n",
    "           \n",
    "        if trace : return res , klds\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generating Processes\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some simple data generating processes that map $(N,I,\\mathbf{i})$ (along with possibly other parameters) to draws of $\\mathbf{y}$. Below we define functions that will randomly draw from a Logit, a Latent Class Logit, and a (Gaussian) Random Coefficients Logit model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multinomial Logit data generating process\n",
    "def mnl_dgp( N , I , i ) : \n",
    "    \n",
    "    pT = 2.0 * rand() - 1.0\n",
    "    \n",
    "    print( pT )\n",
    "    \n",
    "    if pT > 0 : PT = 1.0 / ( 1.0 + np.exp( -pT ) )\n",
    "    else : PT = np.exp( pT ) ; PT = PT / ( 1.0 + PT )\n",
    "    y = 2.0 * ( rand(N) <= PT ) - 1.0\n",
    "    \n",
    "    return y , PT\n",
    "\n",
    "# Latent Class Logit data generating process\n",
    "def lcl_dgp( N , I , i , C ) : \n",
    "    \n",
    "    w = rand(C) ; w = w / w.sum()\n",
    "    pT = 2.0 * rand(C) - 1.0\n",
    "    ic = randc(C,I,w)\n",
    "    eU = np.exp( pT[ ic[i] ] )\n",
    "    PL = eU / ( 1.0 + eU )\n",
    "    y = 2.0 * ( rand(N) <= PL ) - 1.0\n",
    "    \n",
    "    PL = np.exp( pT ) ; PL = PL / ( 1.0 + PL )\n",
    "    PT = np.sum( PL * w )\n",
    "    \n",
    "    return y , PT\n",
    "\n",
    "# Gaussian Random Coefficient Logit data generating process\n",
    "def grc_dgp( N , I , i , mu=None , sigma=None ) : \n",
    "    \n",
    "    pT = rand(2)\n",
    "    if mu is not None : pT[0] = mu\n",
    "    else : pT[0] = 2.0 * pT[0] - 1.0\n",
    "    if sigma is not None : pT[1] = sigma\n",
    "    \n",
    "    print( pT )\n",
    "    \n",
    "    v  = pT[0] + pT[1] * randn(I)\n",
    "    eU = np.exp( v[i] )\n",
    "    PL = eU / ( 1.0 + eU )\n",
    "    y  = 2.0 * ( rand(N) <= PL ) - 1.0\n",
    "    \n",
    "    v   = pT[0] + pT[1] * randn( int(1e6) )\n",
    "    PL  = np.zeros( int(1e6) )\n",
    "    ind = np.where( v >  0.0 )[0] ; PL[ind] = 1.0 / ( 1.0 + np.exp( -v[ind] ) )\n",
    "    ind = np.where( v <= 0.0 )[0] ; PL[ind] = np.exp( v[ind] ) ; PL[ind] = PL[ind] / ( 1.0 + PL[ind] )\n",
    "    PT  = PL.mean()\n",
    "    \n",
    "    return y , PT\n",
    "\n",
    "# Mixture of Gaussians Random Coefficient Logit data generating process\n",
    "def mxg_dgp( N , I , i , C , mu=None , sigma=None ) : \n",
    "    \n",
    "    pT = rand( 2 , C )\n",
    "    if mu is not None : pT[0,:] = mu\n",
    "    else : pT[0,:] = 2.0 * pT[0,:] - 1.0\n",
    "    if sigma is not None : pT[1,:] = sigma\n",
    "    \n",
    "    print( pT )\n",
    "    \n",
    "    w = rand(C) ; w = w / w.sum()\n",
    "    ic = randc(C,I,w)\n",
    "    \n",
    "    v  = pT[0,ic] + pT[1,ic] * randn(I)\n",
    "    \n",
    "    eU = np.exp( v[i] )\n",
    "    PL = eU / ( 1.0 + eU )\n",
    "    y  = 2.0 * ( rand(N) <= PL ) - 1.0\n",
    "    \n",
    "    PT = 0.0\n",
    "    for c in range(C) : \n",
    "        v   = pT[0,c] + pT[1,c] * randn( int(1e6) )\n",
    "        PL  = np.zeros( int(1e6) )\n",
    "        ind = np.where( v >  0.0 )[0] ; PL[ind] = 1.0 / ( 1.0 + np.exp( -v[ind] ) )\n",
    "        ind = np.where( v <= 0.0 )[0] ; PL[ind] = np.exp( v[ind] ) ; PL[ind] = PL[ind] / ( 1.0 + PL[ind] )\n",
    "        PT += w[c] * PL.mean()\n",
    "    \n",
    "    return y , PT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Experiments\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To draw a simulation experiment, we need to choose or draw $(N,I)$, then $i_n \\in \\{1,\\dotsc,I\\}$ for all $n$, and then choice dummies $y_n$ following some data generating process. Below we wrap the second two parts in a single function, `draw_experiment`. Then we create model object instances from the resultant data for each of our model types. \n",
    "\n",
    "Just to make sure everything works, we run a test that just (a) checks the gradient for each model and (b) fits the model on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05859421075185023\n",
      "Logit :: Checking gradient...\n",
      "0.1000000000000000 , 0.0118707807970800\n",
      "0.0100000000000000 , 0.0011949690686022\n",
      "0.0010000000000000 , 0.0001195720337247\n",
      "0.0001000000000000 , 0.0000119579489702\n",
      "0.0000100000000000 , 0.0000011958222249\n",
      "0.0000010000000000 , 0.0000001193721847\n",
      "0.0000001000000000 , 0.0000000142340642\n",
      "0.0000000100000000 , 0.0000000086829491\n",
      "0.0000000010000000 , 0.0000002022594256\n",
      "0.0000000001000000 , 0.0000010904378453\n",
      "Logit :: Fitting model...\n",
      "Logit :: Solver ran in 0.015235 seconds\n",
      "Logit :: Solver message: `gtol` termination condition is satisfied.\n",
      "Logit :: Formatted solution: 0.09\n",
      "Logit :: Kullback-Leibler Divergence at solution : 0.000108\n",
      "Logit :: Relative Average Likelihood at solution : 1.000\n",
      " \n",
      "Latent Class Logit :: Checking gradient...\n",
      "0.1000000000000000 , 0.0134535868935457\n",
      "0.0100000000000000 , 0.0015733983797370\n",
      "0.0010000000000000 , 0.0001602021192235\n",
      "0.0001000000000000 , 0.0000160495943067\n",
      "0.0000100000000000 , 0.0000016052341543\n",
      "0.0000010000000000 , 0.0000001604454212\n",
      "0.0000001000000000 , 0.0000000127857589\n",
      "0.0000000100000000 , 0.0000000198867729\n",
      "0.0000000010000000 , 0.0000002951158073\n",
      "0.0000000001000000 , 0.0000029912716708\n",
      "Latent Class Logit :: Fitting model...\n",
      "Latent Class Logit :: Solver ran in 0.114299 seconds\n",
      "Latent Class Logit :: Solver message: `gtol` termination condition is satisfied.\n",
      "Latent Class Logit :: Formatted solution: ( 0.33 , 0.36 ) , ( 0.33 , 0.33 ) , ( -0.44 , 0.31 )\n",
      "Latent Class Logit :: Kullback-Leibler Divergence at solution : 0.000109\n",
      "Latent Class Logit :: Relative Average Likelihood at solution : 1.000\n",
      " \n",
      "Class Class Logit :: Checking gradient...\n",
      "0.1000000000000000 , 0.1590008032853779\n",
      "0.0100000000000000 , 0.0828110055242132\n",
      "0.0010000000000000 , 0.0158719876994723\n",
      "0.0001000000000000 , 0.0017620636186761\n",
      "0.0000100000000000 , 0.0001781975703603\n",
      "0.0000010000000000 , 0.0000178399209788\n",
      "0.0000001000000000 , 0.0000017837645744\n",
      "0.0000000100000000 , 0.0000001717207427\n",
      "0.0000000010000000 , 0.0000002483099398\n",
      "0.0000000001000000 , 0.0000019339834353\n",
      "Class Class Logit :: Fitting model...\n",
      "Class Class Logit :: Solver ran in 1.309593 seconds\n",
      "Class Class Logit :: Solver message: `gtol` termination condition is satisfied.\n",
      "Class Class Logit :: Formatted solution: ( -0.84 , 0.29 ) , ( 0.09 , 0.40 ) , ( 0.92 , 0.31 )\n",
      "Class Class Logit :: Kullback-Leibler Divergence at solution : 0.000035\n",
      "Class Class Logit :: Relative Average Likelihood at solution : 1.000\n",
      " \n",
      "Gaussian RC Logit (SAA) :: Checking gradient...\n",
      "0.1000000000000000 , 0.0033338095308321\n",
      "0.0100000000000000 , 0.0003338167620980\n",
      "0.0010000000000000 , 0.0000333865124534\n",
      "0.0001000000000000 , 0.0000033387009922\n",
      "0.0000100000000000 , 0.0000003338857067\n",
      "0.0000010000000000 , 0.0000000334260496\n",
      "0.0000001000000000 , 0.0000000048933179\n",
      "0.0000000100000000 , 0.0000000204364402\n",
      "0.0000000010000000 , 0.0000002147634935\n",
      "0.0000000001000000 , 0.0000013860107605\n",
      "Gaussian RC Logit (SAA) :: Fitting model...\n",
      "Gaussian RC Logit (SAA) :: Solver ran in 0.303823 seconds\n",
      "Gaussian RC Logit (SAA) :: Solver message: `gtol` termination condition is satisfied.\n",
      "Gaussian RC Logit (SAA) :: Formatted solution: 0.09 ( 0.33 )\n",
      "Gaussian RC Logit (SAA) :: Kullback-Leibler Divergence at solution : 0.000108\n",
      "Gaussian RC Logit (SAA) :: Relative Average Likelihood at solution : 1.000\n",
      " \n",
      "Gaussian RC Logit (GLQ) :: Checking gradient...\n",
      "0.1000000000000000 , 0.0092069746134118\n",
      "0.0100000000000000 , 0.0009242416769630\n",
      "0.0010000000000000 , 0.0000924581060870\n",
      "0.0001000000000000 , 0.0000092461483183\n",
      "0.0000100000000000 , 0.0000009246170764\n",
      "0.0000010000000000 , 0.0000000925493284\n",
      "0.0000001000000000 , 0.0000000097266908\n",
      "0.0000000100000000 , 0.0000000075356024\n",
      "0.0000000010000000 , 0.0000001256911605\n",
      "0.0000000001000000 , 0.0000015689810925\n",
      "Gaussian RC Logit (GLQ) :: Fitting model...\n",
      "Gaussian RC Logit (GLQ) :: Solver ran in 0.048698 seconds\n",
      "Gaussian RC Logit (GLQ) :: Solver message: `gtol` termination condition is satisfied.\n",
      "Gaussian RC Logit (GLQ) :: Formatted solution: 0.09 ( 0.33 )\n",
      "Gaussian RC Logit (GLQ) :: Kullback-Leibler Divergence at solution : 0.000107\n",
      "Gaussian RC Logit (GLQ) :: Relative Average Likelihood at solution : 1.000\n",
      " \n",
      "idLogit :: Checking gradient...\n",
      "0.1000000000000000 , 0.0501929774470868\n",
      "0.0100000000000000 , 0.0050195475775085\n",
      "0.0010000000000000 , 0.0005019572138467\n",
      "0.0001000000000000 , 0.0000501955919576\n",
      "0.0000100000000000 , 0.0000050184321667\n",
      "0.0000010000000000 , 0.0000005057945012\n",
      "0.0000001000000000 , 0.0000001220120059\n",
      "0.0000000100000000 , 0.0000018880617754\n",
      "0.0000000010000000 , 0.0000191719626295\n",
      "0.0000000001000000 , 0.0001903612083446\n",
      "idLogit :: Fitting model...\n",
      "idLogit :: Solver ran in 0.149772 seconds\n",
      "idLogit :: Solver message: `gtol` termination condition is satisfied.\n",
      "idLogit :: Formatted solution: 0.09 ( 0.00 , 0.0000 )\n",
      "idLogit :: Kullback-Leibler Divergence at solution : 0.000108\n",
      "idLogit :: Relative Average Likelihood at solution : 1.000\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def draw_experiment( N , I , dgp ) : \n",
    "    i = randi( I , N ) ; y , PT = dgp( N , I , i )\n",
    "    return i , y , PT\n",
    "\n",
    "N , I = 1000 , 100\n",
    "i , y , PT = draw_experiment( N , I , mnl_dgp )\n",
    "\n",
    "# define simple Logit\n",
    "mnl = Logit( N , y )\n",
    "\n",
    "# define latent class Logit instances, for a specific number of modeled classes\n",
    "M = 3\n",
    "lcl = LatentClassLogit( N , I , i , y , M )\n",
    "ccl = ClassClassLogit( N , I , i , y , M )\n",
    "\n",
    "# define sample average and gauss-legendre random coefficient Logit instances\n",
    "saa = GRCLogitSAA( N , I , i , y )\n",
    "glq = GRCLogitGLQ( N , I , i , y )\n",
    "\n",
    "# finally, define an idLogit instance\n",
    "idl = idLogit( N , I , i , y )\n",
    "\n",
    "def basic_test( prob ) : \n",
    "    print( \"%s :: Checking gradient...\" % ( prob.type ) )\n",
    "    prob.grad_check(  )\n",
    "    print( \"%s :: Fitting model...\" % ( prob.type ) )\n",
    "    soln = prob.fit().soln\n",
    "    print( \"%s :: Solver ran in %0.6f seconds\" % ( prob.type , soln.solvertime ) )\n",
    "    print( \"%s :: Solver message: %s\" % ( prob.type , soln.message ) )\n",
    "    print( \"%s :: Formatted solution: %s\" % ( prob.type , prob.printx() ) )\n",
    "    kld = prob.kld( PT )\n",
    "    print( \"%s :: Kullback-Leibler Divergence at solution : %0.6f\" % ( prob.type , kld ) )\n",
    "    print( \"%s :: Relative Average Likelihood at solution : %0.3f\" % ( prob.type , np.exp( - kld ) ) )\n",
    "    \n",
    "basic_test( mnl ) ; print(\" \")\n",
    "basic_test( lcl ) ; print(\" \")\n",
    "basic_test( ccl ) ; print(\" \")\n",
    "basic_test( saa ) ; print(\" \")\n",
    "basic_test( glq ) ; print(\" \")\n",
    "basic_test( idl ) ; print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have some confidence that everything actually works, we can run some _real_ tests. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.83505144 10.        ]\n",
      "[[1.00091372e-01 1.06720213e-02]\n",
      " [1.00239392e-01 1.06720535e-02]\n",
      " [1.00239392e-01 1.06720535e-02]\n",
      " [1.00627949e-01 1.06719130e-02]\n",
      " [1.00627949e-01 1.06719130e-02]\n",
      " [1.01652356e-01 1.06720346e-02]\n",
      " [1.01652356e-01 1.06720346e-02]\n",
      " [1.04383948e-01 1.06721343e-02]\n",
      " [1.04383948e-01 1.06721343e-02]\n",
      " [1.11888052e-01 1.06721516e-02]\n",
      " [1.11888052e-01 1.06721516e-02]\n",
      " [1.34189155e-01 1.06720333e-02]\n",
      " [1.34189155e-01 1.06720333e-02]\n",
      " [1.60935229e-01 1.06718036e-02]\n",
      " [3.47551896e-01 1.06721467e-02]\n",
      " [7.50564816e-01 1.06722258e-02]\n",
      " [7.50564816e-01 1.06719128e-02]\n",
      " [1.95792507e+01 1.06720498e-02]\n",
      " [1.95792507e+01 1.06720156e-02]\n",
      " [5.10744775e+02 1.06719773e-02]\n",
      " [1.00000000e+05 1.06720366e-02]\n",
      " [1.00000000e+05 1.06720366e-02]]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='e7aa2938-255f-4d42-a56d-0ee67ee27355'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a1bb3ee10>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "N , I = int(1e5) , 100\n",
    "i , y , PT = draw_experiment( N , I , lambda N , I , i : grc_dgp(N,I,i,sigma=10.0) )\n",
    "# i , y , PT = draw_experiment( N , I , lambda N,I,i : lcl_dgp(N,I,i,3) )\n",
    "\n",
    "prob = idLogit( N , I , i , y )\n",
    "\n",
    "lint , klds = prob.bestLambdas10( PT , alpha=0.0 )\n",
    "klds = np.array( klds )\n",
    "inds = np.argsort( klds[:,0] )\n",
    "klds = klds[inds,:]\n",
    "\n",
    "print( klds )\n",
    "\n",
    "plt.figure()\n",
    "plt.loglog( klds[:,0] , klds[:,1] , '.k' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code helps us abstract away from the mechanics of doing the tests themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_model_object( code , N , I , i , y , data={} ) : \n",
    "    \"\"\"\n",
    "    This is a convenience function for instantiating model objects from a \n",
    "    code, basic data shared by all models, and optional additional data that\n",
    "    a class might require or use. \n",
    "    \"\"\"\n",
    "    if data is None : data = {} # generic empty object for use \n",
    "    if   code == \"mnl\" : return Logit( N , y[:N] )\n",
    "    elif code == \"lcl\" : return LatentClassLogit( N , I , i[:N] , y[:N] , **data )\n",
    "    elif code == \"ccl\" : return ClassClassLogit( N , I , i[:N] , y[:N] , **data )\n",
    "    elif code == \"saa\" : return GRCLogitSAA( N , I , i[:N] , y[:N] )\n",
    "    elif code == \"glq\" : return GRCLogitGLQ( N , I , i[:N] , y[:N] )\n",
    "    elif code == \"idl\" : return idLogit( N , I , i[:N] , y[:N] , **data )\n",
    "    else : return None\n",
    "    \n",
    "def fit_model( prob , p0=None , PT=None , timeout=None , verbose=True ) : \n",
    "    \n",
    "    res = { 'N' : prob.N , 'p' : None , 'x' : None , 'k' : None , \\\n",
    "               't' : None , 'e' : True , 'm' : None , 's' : None }\n",
    "    \n",
    "    try : \n",
    "        prob.fit( maxtime=timeout , p0=p0 )\n",
    "        if prob.soln is None    : raise Exception( \"indeterminate solver error\" )\n",
    "        if prob.soln['timeout'] : raise Exception( \"solver timeout\" )\n",
    "        if prob.soln['error']   : raise prob.soln['message']\n",
    "        res['s'] = prob.soln['status']\n",
    "        res['p'] = prob.printx()\n",
    "        res['x'] = prob.getx()\n",
    "        res['t'] = prob.soln['solvertime']\n",
    "        res['e'] = prob.soln['error']\n",
    "        res['m'] = prob.soln['message']\n",
    "        if ( PT is not None ) and ( not prob.soln['error'] ) : res['k'] = prob.kld( PT )\n",
    "        if verbose : \n",
    "            print( \"%s :: Solve attempt finished (%i) for N = %i\" \\\n",
    "                      % ( prob.type , prob.soln['status'] , prob.N ) )\n",
    "            \n",
    "    except Exception as e : \n",
    "        if verbose : print( \"%s :: solve exception occurred: %s\" % ( prob.type , e ) )\n",
    "        try : solvertime = prob.soln['solvertime']\n",
    "        except AttributeError as ae : solvertime = None\n",
    "        res['t'] , res['e'] , res['m'] = solvertime , True , e\n",
    "        \n",
    "    return res\n",
    "    \n",
    "def fit_sequence( Ns , I , i , y , code , data , PT=None , timeout=None , chain=True , verbose=True ) :\n",
    "    \"\"\"\n",
    "    This is a convenience function for running a sequence of fits over \n",
    "    increasing numbers of observations with structured initial conditions. \n",
    "    We can either \"chain\" the solves by passing the solution for one problem\n",
    "    to the next, or not and keep the same inital condition\n",
    "    \"\"\"\n",
    "    results , prob_type , p0 = [] , None , None\n",
    "    for N in Ns : \n",
    "        prob , res = None , None\n",
    "        try :\n",
    "            prob = create_model_object( code , N , I , i , y , data )\n",
    "            if prob_type is None : prob_type = prob.type\n",
    "        except Exception as e : \n",
    "            print( \"Error creating model object: \" , e )\n",
    "        if prob is not None : \n",
    "            res = fit_model( prob , p0=p0 , PT=PT , timeout=timeout , verbose=verbose )\n",
    "            if not res['e'] : \n",
    "                p0 = prob.soln['x'] if chain else prob.soln['x0']\n",
    "        results.append( res )\n",
    "    return results, prob_type\n",
    "\n",
    "class FitSequenceTrial( object ) :\n",
    "    \"\"\"\n",
    "    This is a wrapper class to enable parallelization with pool.map by \n",
    "    making a callable object instance storing the desired model data. \n",
    "    \"\"\"\n",
    "    def __init__( self , Ns , I , i , y , PT=None , timeout=None , verbose=None ) : \n",
    "        self.Ns , self.I , self.i , self.y = Ns , I , i , y\n",
    "        self.timeout , self.verbose = timeout , verbose\n",
    "    def __call__( self , params ) : # expects params = { code , data , trial }  \n",
    "        return fit_sequence( self.Ns , self.I , self.i , self.y , params['code'] , params['data'] , \\\n",
    "                                PT=PT , timeout=self.timeout , verbose=self.verbose )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a class `ModelFitExp` to facilitate running experiments over the sample size ($N$) -- with the same observsational data -- for a variety of models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class ModelFitExp( object ) : \n",
    "    \n",
    "    def __init__( self , I=0 , dgp=None , obsvnums=None , timeout=None , \\\n",
    "                     models=None , trials=None , processes=None , verbose=True ) : \n",
    "        self.I = I \n",
    "        self.dgp = dgp\n",
    "        self.timeout = timeout\n",
    "        self.models = {}\n",
    "        self.verbose = verbose\n",
    "        self.T = trials if trials is not None else 1 # one trial by default\n",
    "        if models is not None : \n",
    "            for m in models : \n",
    "                self.add_model( m['code'] , data=( m['data'] if 'data' in m else None ) )\n",
    "        self.results = {}\n",
    "        self.Ns = []\n",
    "        if obsvnums is None : \n",
    "            self.Ns = [ int(N) for N in 10.0 ** np.arange( 3 , 6.1 , 0.1 ) ]\n",
    "        else : \n",
    "            self.Ns = obsvnums\n",
    "        self.P = 1\n",
    "        self.parallelize( processes )\n",
    "        self.iy_saved = False\n",
    "    \n",
    "    def be_verbose( self ) : \n",
    "        self.verbose = True\n",
    "        \n",
    "    def be_quiet( self ) : \n",
    "        self.verbose = False\n",
    "    \n",
    "    def set_obsnums( self , Nmin , Nmax , Nnum ) : \n",
    "        try : \n",
    "            Ndel = ( Nmax - Nmin ) / Nnum\n",
    "            self.Ns = [ int(N) for N in 10.0 ** np.arange( Nmin , Nmax + 0.5*Ndel , Ndel ) ]\n",
    "        except Exception as e : \n",
    "            self.Ns = None\n",
    "            raise e\n",
    "            \n",
    "    def set_dgp( self , dgp ) : \n",
    "        self.dgp = dgp\n",
    "    \n",
    "    def set_timeout( self , timeout ) : \n",
    "        self.timeout = timeout\n",
    "        \n",
    "    def add_model( self , code , data=None ) : \n",
    "        mid = random_string( 16 )\n",
    "        self.models[mid] = { 'code' : code , 'data' : data }\n",
    "        return mid\n",
    "        \n",
    "    def del_model( self , mid ) : \n",
    "        if mid in self.models : \n",
    "            del self.models[mid]\n",
    "            \n",
    "    def clear_models( self ) : \n",
    "        del self.models\n",
    "        self.models = {}\n",
    "        \n",
    "    def reset_results( self ) : \n",
    "        del self.results\n",
    "        self.results = {}\n",
    "        \n",
    "    def parallelize( self , procs ) : \n",
    "        try : self.P = int( procs )\n",
    "        except Exception as e : pass\n",
    "        if self.P < 1 : self.P = 1\n",
    "        \n",
    "    def print_models( self ) : \n",
    "        for k in self.models : \n",
    "            print( \"(%s) %s %s\" % ( k , self.models[k]['code'] , \\\n",
    "                                   str(self.models[k]['data']) if self.models[k]['data'] is not None else \"\" ) )\n",
    "        \n",
    "    def print_results( self ) : \n",
    "        for k in self.results : \n",
    "            print( \"%s%s\" % ( self.models[k]['type'] , \"\" if self.models[k]['data'] is None \\\n",
    "                                 else \", %s\" % str(self.models[k]['data']) ) )\n",
    "            t = 1\n",
    "            for trial in self.results[k] : \n",
    "                for i in trial : \n",
    "                    if i['e'] : \n",
    "                        print( \"%i , %i (%s) , %s\" \\\n",
    "                                  % ( t , i['N'] , \"-\" if i['s'] is None else \"%i\" % i['s'] , i['m'] ) )\n",
    "                    else :\n",
    "                        print( \"%i , %i (%s) , %0.2fs , %0.3f , %s \" \\\n",
    "                                  % ( t , i['N'] , \"-\" if i['s'] is None else \"%i\" % i['s'] , \\\n",
    "                                       i['t'] , np.exp( - i['k'] ) , i['p'] ) )\n",
    "                print( \" \" )\n",
    "                t += 1\n",
    "    \n",
    "    def run( self , trials=None , save=False , resample=False ) : \n",
    "        \n",
    "        if trials is not None : self.T = trials\n",
    "        if self.iy_saved and ( not resample ) : \n",
    "            i , y = self.i , self.y\n",
    "        else : \n",
    "            i , y , self.PT = draw_experiment( self.Ns[-1] , self.I , self.dgp )\n",
    "            if save : \n",
    "                self.i , self.y , self.iy_saved = i , y , True\n",
    "                \n",
    "        # parallelize? May not be worthwhile. \n",
    "        if self.P > 1 : \n",
    "            map_params = []\n",
    "            for k in self.models : \n",
    "                if k not in self.results : self.results[k] = [ [] for t in range(self.T) ]\n",
    "                for t in range( self.T ) : \n",
    "                    map_params.append( { \n",
    "                        'code'  : self.models[k]['code'] ,\n",
    "                        'data'  : self.models[k]['data'] ,\n",
    "                        'trial' : t\n",
    "                    } )\n",
    "            with ThreadPool( processes=self.P ) as pool : \n",
    "                FST = FitSequenceTrial( self.Ns , self.I , i , y , \\\n",
    "                                           PT=self.PT , timeout=self.timeout , verbose=self.verbose )\n",
    "                results = pool.map( FST , map_params )\n",
    "            j = 0\n",
    "            for k in self.models : \n",
    "                for t in range( self.T ) : \n",
    "                    self.results[k][t] , self.models[k]['type'] = results[j][0] , results[j][1]\n",
    "                    j += 1\n",
    "                    \n",
    "        # serial trials; seems to be the best\n",
    "        else : \n",
    "            for k in self.models : \n",
    "                if k not in self.results : self.results[k] = []\n",
    "                for t in range( self.T ) : \n",
    "                    while t >= len( self.results[k] ) : self.results[k].append( [] )\n",
    "                    self.results[k][t] , self.models[k]['type'] \\\n",
    "                        = fit_sequence( self.Ns , self.I , i , y , \\\n",
    "                                        self.models[k]['code'] , self.models[k]['data'] , \\\n",
    "                                        PT=self.PT , timeout=self.timeout , verbose=self.verbose )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit data\n",
    "\n",
    "Let's take the simplest case in the data, the Logit DGP, but try out each model with more and more data. This will help us _start_ to gauge comparative efficiency. To start, we will apply a one minute (60 second) timeout on the fit attempts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7770632070899006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/morrowwr/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:62: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit\n",
      "1 , 1000 (1) , 0.05s , 0.999 , 0.88 \n",
      "1 , 1258 (1) , 0.03s , 1.000 , 0.82 \n",
      "1 , 1584 (1) , 0.03s , 1.000 , 0.83 \n",
      "1 , 1995 (1) , 0.03s , 1.000 , 0.84 \n",
      "1 , 2511 (1) , 0.03s , 0.999 , 0.85 \n",
      "1 , 3162 (1) , 0.03s , 1.000 , 0.84 \n",
      "1 , 3981 (1) , 0.03s , 1.000 , 0.82 \n",
      "1 , 5011 (1) , 0.03s , 1.000 , 0.79 \n",
      "1 , 6309 (1) , 0.03s , 1.000 , 0.80 \n",
      "1 , 7943 (1) , 0.03s , 1.000 , 0.79 \n",
      "1 , 10000 (1) , 0.03s , 1.000 , 0.79 \n",
      " \n",
      "2 , 1000 (1) , 0.03s , 0.999 , 0.88 \n",
      "2 , 1258 (1) , 0.03s , 1.000 , 0.82 \n",
      "2 , 1584 (1) , 0.02s , 1.000 , 0.83 \n",
      "2 , 1995 (1) , 0.03s , 1.000 , 0.84 \n",
      "2 , 2511 (1) , 0.03s , 0.999 , 0.85 \n",
      "2 , 3162 (1) , 0.03s , 1.000 , 0.84 \n",
      "2 , 3981 (1) , 0.03s , 1.000 , 0.82 \n",
      "2 , 5011 (1) , 0.02s , 1.000 , 0.79 \n",
      "2 , 6309 (1) , 0.03s , 1.000 , 0.80 \n",
      "2 , 7943 (1) , 0.02s , 1.000 , 0.79 \n",
      "2 , 10000 (1) , 0.03s , 1.000 , 0.79 \n",
      " \n",
      "Latent Class Logit, {'C': 3}\n",
      "1 , 1000 (1) , 0.07s , 0.999 , ( 0.88 , 0.73 ) , ( 0.88 , 0.27 ) , ( -1.17 , 0.00 ) \n",
      "1 , 1258 (-) , solver timeout\n",
      "1 , 1584 (1) , 0.08s , 1.000 , ( 0.83 , 0.70 ) , ( 0.83 , 0.30 ) , ( -1.29 , 0.00 ) \n",
      "1 , 1995 (1) , 0.07s , 1.000 , ( 0.84 , 0.70 ) , ( 0.84 , 0.30 ) , ( -1.46 , 0.00 ) \n",
      "1 , 2511 (1) , 0.08s , 0.999 , ( 0.85 , 0.70 ) , ( 0.85 , 0.30 ) , ( -1.51 , 0.00 ) \n",
      "1 , 3162 (1) , 0.06s , 1.000 , ( 0.84 , 0.70 ) , ( 0.84 , 0.30 ) , ( -1.51 , 0.00 ) \n",
      "1 , 3981 (1) , 0.06s , 1.000 , ( 0.82 , 0.70 ) , ( 0.82 , 0.30 ) , ( -1.52 , 0.00 ) \n",
      "1 , 5011 (1) , 0.08s , 1.000 , ( 0.79 , 0.69 ) , ( 0.79 , 0.30 ) , ( -1.52 , 0.00 ) \n",
      "1 , 6309 (1) , 0.06s , 1.000 , ( 0.80 , 0.69 ) , ( 0.80 , 0.30 ) , ( -1.52 , 0.00 ) \n",
      "1 , 7943 (1) , 0.06s , 1.000 , ( 0.79 , 0.69 ) , ( 0.79 , 0.31 ) , ( -1.52 , 0.00 ) \n",
      "1 , 10000 (1) , 0.06s , 1.000 , ( 0.79 , 0.69 ) , ( 0.79 , 0.31 ) , ( -1.52 , 0.00 ) \n",
      " \n",
      "2 , 1000 (1) , 0.09s , 0.999 , ( 0.88 , 0.57 ) , ( 0.88 , 0.33 ) , ( 0.88 , 0.10 ) \n",
      "2 , 1258 (1) , 0.06s , 1.000 , ( 0.82 , 0.61 ) , ( 0.82 , 0.30 ) , ( 0.82 , 0.10 ) \n",
      "2 , 1584 (1) , 0.07s , 1.000 , ( 0.83 , 0.61 ) , ( 0.83 , 0.30 ) , ( 0.83 , 0.09 ) \n",
      "2 , 1995 (1) , 0.09s , 1.000 , ( 0.84 , 0.61 ) , ( 0.84 , 0.30 ) , ( 0.84 , 0.09 ) \n",
      "2 , 2511 (1) , 0.08s , 0.999 , ( 0.85 , 0.61 ) , ( 0.85 , 0.30 ) , ( 0.85 , 0.09 ) \n",
      "2 , 3162 (1) , 0.08s , 1.000 , ( 0.84 , 0.61 ) , ( 0.84 , 0.30 ) , ( 0.84 , 0.09 ) \n",
      "2 , 3981 (1) , 0.06s , 1.000 , ( 0.82 , 0.61 ) , ( 0.82 , 0.30 ) , ( 0.82 , 0.10 ) \n",
      "2 , 5011 (1) , 0.07s , 1.000 , ( 0.79 , 0.61 ) , ( 0.79 , 0.30 ) , ( 0.79 , 0.10 ) \n",
      "2 , 6309 (1) , 0.08s , 1.000 , ( 0.80 , 0.61 ) , ( 0.80 , 0.29 ) , ( 0.80 , 0.10 ) \n",
      "2 , 7943 (1) , 0.08s , 1.000 , ( 0.79 , 0.61 ) , ( 0.79 , 0.30 ) , ( 0.79 , 0.09 ) \n",
      "2 , 10000 (1) , 0.10s , 1.000 , ( 0.79 , 0.61 ) , ( 0.79 , 0.30 ) , ( 0.79 , 0.09 ) \n",
      " \n",
      "Class Class Logit, {'C': 3}\n",
      "1 , 1000 (1) , 2.98s , 0.999 , ( 0.16 , 0.27 ) , ( 1.78 , 0.27 ) , ( 0.87 , 0.46 ) \n",
      "1 , 1258 (1) , 2.66s , 1.000 , ( 0.29 , 0.32 ) , ( 1.61 , 0.25 ) , ( 0.87 , 0.43 ) \n",
      "1 , 1584 (1) , 1.57s , 1.000 , ( 0.31 , 0.26 ) , ( 1.44 , 0.24 ) , ( 0.86 , 0.50 ) \n",
      "1 , 1995 (1) , 1.45s , 0.999 , ( 0.33 , 0.24 ) , ( 1.44 , 0.28 ) , ( 0.84 , 0.48 ) \n",
      "1 , 2511 (1) , 3.37s , 0.999 , ( 0.37 , 0.28 ) , ( 0.85 , 0.36 ) , ( 1.30 , 0.36 ) \n",
      "1 , 3162 (1) , 4.68s , 1.000 , ( 0.37 , 0.22 ) , ( 0.86 , 0.54 ) , ( 1.32 , 0.24 ) \n",
      "1 , 3981 (1) , 4.32s , 1.000 , ( 0.43 , 0.25 ) , ( 0.82 , 0.42 ) , ( 1.15 , 0.34 ) \n",
      "1 , 5011 (1) , 8.21s , 1.000 , ( 0.47 , 0.28 ) , ( 0.81 , 0.48 ) , ( 1.18 , 0.23 ) \n",
      "1 , 6309 (1) , 1.25s , 1.000 , ( 0.48 , 0.27 ) , ( 0.81 , 0.47 ) , ( 1.14 , 0.26 ) \n",
      "1 , 7943 (1) , 0.82s , 1.000 , ( 0.54 , 0.31 ) , ( 0.80 , 0.39 ) , ( 1.06 , 0.29 ) \n",
      "1 , 10000 (1) , 0.60s , 1.000 , ( 0.58 , 0.31 ) , ( 0.79 , 0.37 ) , ( 1.02 , 0.31 ) \n",
      " \n",
      "2 , 1000 (1) , 3.78s , 1.000 , ( 0.02 , 0.20 ) , ( 1.78 , 0.27 ) , ( 0.81 , 0.53 ) \n",
      "2 , 1258 (1) , 3.36s , 1.000 , ( 0.29 , 0.32 ) , ( 1.61 , 0.25 ) , ( 0.87 , 0.43 ) \n",
      "2 , 1584 (1) , 5.57s , 1.000 , ( 0.28 , 0.23 ) , ( 1.44 , 0.24 ) , ( 0.85 , 0.53 ) \n",
      "2 , 1995 (1) , 2.33s , 0.999 , ( 0.33 , 0.24 ) , ( 1.44 , 0.28 ) , ( 0.84 , 0.48 ) \n",
      "2 , 2511 (1) , 0.90s , 0.999 , ( 0.37 , 0.28 ) , ( 1.30 , 0.36 ) , ( 0.85 , 0.36 ) \n",
      "2 , 3162 (1) , 3.56s , 1.000 , ( 0.36 , 0.22 ) , ( 1.28 , 0.29 ) , ( 0.84 , 0.49 ) \n",
      "2 , 3981 (1) , 2.73s , 1.000 , ( 0.43 , 0.25 ) , ( 1.15 , 0.35 ) , ( 0.81 , 0.41 ) \n",
      "2 , 5011 (1) , 7.12s , 1.000 , ( 0.46 , 0.27 ) , ( 1.18 , 0.23 ) , ( 0.81 , 0.50 ) \n",
      "2 , 6309 (1) , 1.69s , 1.000 , ( 0.48 , 0.27 ) , ( 1.15 , 0.25 ) , ( 0.82 , 0.49 ) \n",
      "2 , 7943 (1) , 0.81s , 1.000 , ( 0.54 , 0.31 ) , ( 1.06 , 0.28 ) , ( 0.80 , 0.40 ) \n",
      "2 , 10000 (1) , 2.34s , 1.000 , ( 0.57 , 0.31 ) , ( 1.03 , 0.29 ) , ( 0.80 , 0.40 ) \n",
      " \n",
      "Gaussian RC Logit (SAA)\n",
      "1 , 1000 (1) , 0.35s , 0.999 , 0.88 ( 0.01 ) \n",
      "1 , 1258 (1) , 0.37s , 1.000 , 0.82 ( 0.01 ) \n",
      "1 , 1584 (1) , 0.47s , 1.000 , 0.83 ( 0.00 ) \n",
      "1 , 1995 (1) , 0.59s , 1.000 , 0.84 ( 0.01 ) \n",
      "1 , 2511 (1) , 0.63s , 0.999 , 0.85 ( 0.01 ) \n",
      "1 , 3162 (1) , 0.83s , 1.000 , 0.84 ( 0.01 ) \n",
      "1 , 3981 (1) , 1.03s , 1.000 , 0.82 ( 0.01 ) \n",
      "1 , 5011 (1) , 1.33s , 1.000 , 0.79 ( 0.01 ) \n",
      "1 , 6309 (1) , 1.74s , 1.000 , 0.80 ( 0.01 ) \n",
      "1 , 7943 (1) , 1.81s , 1.000 , 0.79 ( 0.01 ) \n",
      "1 , 10000 (1) , 2.07s , 1.000 , 0.79 ( 0.02 ) \n",
      " \n",
      "2 , 1000 (1) , 0.32s , 0.999 , 0.88 ( 0.01 ) \n",
      "2 , 1258 (1) , 0.42s , 1.000 , 0.82 ( 0.01 ) \n",
      "2 , 1584 (1) , 0.48s , 1.000 , 0.83 ( 0.00 ) \n",
      "2 , 1995 (1) , 0.63s , 1.000 , 0.84 ( 0.01 ) \n",
      "2 , 2511 (1) , 0.74s , 0.999 , 0.85 ( 0.01 ) \n",
      "2 , 3162 (1) , 0.74s , 1.000 , 0.84 ( 0.01 ) \n",
      "2 , 3981 (1) , 0.88s , 1.000 , 0.82 ( 0.01 ) \n",
      "2 , 5011 (1) , 1.16s , 1.000 , 0.79 ( 0.01 ) \n",
      "2 , 6309 (1) , 1.68s , 1.000 , 0.80 ( 0.01 ) \n",
      "2 , 7943 (1) , 1.70s , 1.000 , 0.79 ( 0.01 ) \n",
      "2 , 10000 (1) , 2.31s , 1.000 , 0.79 ( 0.01 ) \n",
      " \n",
      "Gaussian RC Logit (GLQ)\n",
      "1 , 1000 (1) , 0.10s , 0.999 , 0.88 ( 0.01 ) \n",
      "1 , 1258 (1) , 0.09s , 1.000 , 0.82 ( 0.01 ) \n",
      "1 , 1584 (1) , 0.09s , 1.000 , 0.83 ( 0.00 ) \n",
      "1 , 1995 (1) , 0.08s , 1.000 , 0.84 ( 0.01 ) \n",
      "1 , 2511 (1) , 0.10s , 0.999 , 0.85 ( 0.01 ) \n",
      "1 , 3162 (1) , 0.10s , 1.000 , 0.84 ( 0.01 ) \n",
      "1 , 3981 (1) , 0.09s , 1.000 , 0.82 ( 0.01 ) \n",
      "1 , 5011 (1) , 0.16s , 1.000 , 0.79 ( 0.01 ) \n",
      "1 , 6309 (1) , 0.19s , 1.000 , 0.80 ( 0.02 ) \n",
      "1 , 7943 (1) , 0.17s , 1.000 , 0.79 ( 0.01 ) \n",
      "1 , 10000 (1) , 0.19s , 1.000 , 0.79 ( 0.01 ) \n",
      " \n",
      "2 , 1000 (1) , 0.09s , 0.999 , 0.88 ( 0.01 ) \n",
      "2 , 1258 (1) , 0.08s , 1.000 , 0.82 ( 0.01 ) \n",
      "2 , 1584 (1) , 0.08s , 1.000 , 0.83 ( 0.00 ) \n",
      "2 , 1995 (1) , 0.08s , 1.000 , 0.84 ( 0.01 ) \n",
      "2 , 2511 (1) , 0.09s , 0.999 , 0.85 ( 0.01 ) \n",
      "2 , 3162 (1) , 0.09s , 1.000 , 0.84 ( 0.01 ) \n",
      "2 , 3981 (1) , 0.09s , 1.000 , 0.82 ( 0.01 ) \n",
      "2 , 5011 (1) , 0.17s , 1.000 , 0.79 ( 0.01 ) \n",
      "2 , 6309 (1) , 0.19s , 1.000 , 0.80 ( 0.02 ) \n",
      "2 , 7943 (1) , 0.21s , 1.000 , 0.79 ( 0.01 ) \n",
      "2 , 10000 (1) , 0.24s , 1.000 , 0.79 ( 0.01 ) \n",
      " \n",
      "idLogit\n",
      "1 , 1000 (1) , 0.22s , 0.999 , 0.88 ( 0.00 , 0.0004 ) \n",
      "1 , 1258 (1) , 0.13s , 1.000 , 0.82 ( 0.00 , 0.0915 ) \n",
      "1 , 1584 (1) , 0.13s , 1.000 , 0.83 ( 0.00 , 0.5365 ) \n",
      "1 , 1995 (1) , 0.15s , 1.000 , 0.84 ( 0.00 , 0.4593 ) \n",
      "1 , 2511 (1) , 0.13s , 0.999 , 0.85 ( 0.00 , 0.1775 ) \n",
      "1 , 3162 (1) , 0.13s , 1.000 , 0.84 ( 0.00 , 0.2477 ) \n",
      "1 , 3981 (1) , 0.15s , 1.000 , 0.82 ( 0.00 , 0.1582 ) \n",
      "1 , 5011 (1) , 0.15s , 1.000 , 0.79 ( 0.00 , 0.0545 ) \n",
      "1 , 6309 (1) , 0.15s , 1.000 , 0.80 ( 0.00 , 0.2684 ) \n",
      "1 , 7943 (1) , 0.16s , 1.000 , 0.79 ( 0.00 , 0.2370 ) \n",
      "1 , 10000 (1) , 0.18s , 1.000 , 0.79 ( 0.00 , 0.7331 ) \n",
      " \n",
      "2 , 1000 (1) , 0.18s , 0.999 , 0.88 ( 0.00 , 0.0000 ) \n",
      "2 , 1258 (1) , 0.13s , 1.000 , 0.82 ( 0.00 , 0.0898 ) \n",
      "2 , 1584 (1) , 0.13s , 1.000 , 0.83 ( 0.00 , 0.5364 ) \n",
      "2 , 1995 (1) , 0.16s , 1.000 , 0.84 ( 0.00 , 0.4593 ) \n",
      "2 , 2511 (1) , 0.13s , 0.999 , 0.85 ( 0.00 , 0.1775 ) \n",
      "2 , 3162 (1) , 0.13s , 1.000 , 0.84 ( 0.00 , 0.2477 ) \n",
      "2 , 3981 (1) , 0.14s , 1.000 , 0.82 ( 0.00 , 0.1582 ) \n",
      "2 , 5011 (1) , 0.15s , 1.000 , 0.79 ( 0.00 , 0.0545 ) \n",
      "2 , 6309 (1) , 0.15s , 1.000 , 0.80 ( 0.00 , 0.2684 ) \n",
      "2 , 7943 (1) , 0.16s , 1.000 , 0.79 ( 0.00 , 0.2370 ) \n",
      "2 , 10000 (1) , 0.16s , 1.000 , 0.79 ( 0.00 , 0.7331 ) \n",
      " \n",
      "idLogit, {'Lambda1': 10.0, 'Lambda2': 1.0}\n",
      "1 , 1000 (1) , 0.15s , 0.999 , 0.88 ( 0.00 , 0.8939 ) \n",
      "1 , 1258 (1) , 0.20s , 1.000 , 0.82 ( 0.00 , 0.0000 ) \n",
      "1 , 1584 (1) , 0.22s , 1.000 , 0.83 ( 0.00 , 0.9918 ) \n",
      "1 , 1995 (1) , 0.40s , 1.000 , 0.84 ( 0.00 , 0.0165 ) \n",
      "1 , 2511 (1) , 0.25s , 0.999 , 0.85 ( 0.00 , 0.0000 ) \n",
      "1 , 3162 (1) , 0.20s , 1.000 , 0.84 ( 0.00 , 0.0000 ) \n",
      "1 , 3981 (1) , 0.16s , 1.000 , 0.82 ( 0.00 , 0.0000 ) \n",
      "1 , 5011 (1) , 0.16s , 1.000 , 0.79 ( 0.00 , 0.0000 ) \n",
      "1 , 6309 (1) , 0.17s , 1.000 , 0.80 ( 0.00 , 0.0000 ) \n",
      "1 , 7943 (1) , 0.16s , 1.000 , 0.79 ( 0.00 , 0.0000 ) \n",
      "1 , 10000 (1) , 0.17s , 1.000 , 0.79 ( 0.00 , 0.0952 ) \n",
      " \n",
      "2 , 1000 (1) , 0.15s , 0.999 , 0.88 ( 0.00 , 0.0000 ) \n",
      "2 , 1258 (1) , 0.20s , 1.000 , 0.82 ( 0.00 , 0.0000 ) \n",
      "2 , 1584 (1) , 0.22s , 1.000 , 0.83 ( 0.00 , 0.9916 ) \n",
      "2 , 1995 (1) , 0.24s , 1.000 , 0.84 ( 0.00 , 0.0001 ) \n",
      "2 , 2511 (1) , 0.47s , 0.999 , 0.85 ( 0.00 , 0.0000 ) \n",
      "2 , 3162 (1) , 0.27s , 1.000 , 0.84 ( 0.00 , 0.3385 ) \n",
      "2 , 3981 (1) , 0.16s , 1.000 , 0.82 ( 0.00 , 0.0000 ) \n",
      "2 , 5011 (1) , 0.16s , 1.000 , 0.79 ( 0.00 , 0.0000 ) \n",
      "2 , 6309 (1) , 0.16s , 1.000 , 0.80 ( 0.00 , 0.0000 ) \n",
      "2 , 7943 (1) , 0.15s , 1.000 , 0.79 ( 0.00 , 0.0000 ) \n",
      "2 , 10000 (1) , 0.15s , 1.000 , 0.79 ( 0.00 , 0.0952 ) \n",
      " \n",
      "idLogit, {'Lambda1': 1.0, 'Lambda2': 1.0}\n",
      "1 , 1000 (1) , 0.29s , 0.999 , 0.88 ( 0.00 , 0.5083 ) \n",
      "1 , 1258 (1) , 0.12s , 1.000 , 0.82 ( 0.00 , 0.0667 ) \n",
      "1 , 1584 (1) , 0.92s , 1.000 , 0.83 ( 0.00 , 0.0000 ) \n",
      "1 , 1995 (1) , 0.45s , 1.000 , 0.84 ( 0.00 , 0.0000 ) \n",
      "1 , 2511 (1) , 0.19s , 0.999 , 0.85 ( 0.00 , 0.0099 ) \n",
      "1 , 3162 (1) , 0.81s , 1.000 , 0.84 ( 0.00 , 0.2303 ) \n",
      "1 , 3981 (2) , 0.77s , 1.000 , 0.82 ( 0.02 , 0.1381 ) \n",
      "1 , 5011 (1) , 0.35s , 1.000 , 0.79 ( 0.01 , 0.0000 ) \n",
      "1 , 6309 (1) , 6.50s , 1.000 , 0.80 ( 0.03 , 0.0553 ) \n",
      "1 , 7943 (1) , 0.50s , 1.000 , 0.79 ( 0.03 , 0.0000 ) \n",
      "1 , 10000 (2) , 1.41s , 1.000 , 0.79 ( 0.08 , 0.0000 ) \n",
      " \n",
      "2 , 1000 (1) , 0.25s , 0.999 , 0.88 ( 0.00 , 0.0000 ) \n",
      "2 , 1258 (1) , 0.13s , 1.000 , 0.82 ( 0.00 , 0.3616 ) \n",
      "2 , 1584 (1) , 2.65s , 1.000 , 0.83 ( 0.00 , 0.0000 ) \n",
      "2 , 1995 (1) , 0.52s , 1.000 , 0.84 ( 0.00 , 0.0000 ) \n",
      "2 , 2511 (1) , 0.14s , 0.999 , 0.85 ( 0.01 , 0.2694 ) \n",
      "2 , 3162 (1) , 7.95s , 1.000 , 0.84 ( 0.00 , 0.5555 ) \n",
      "2 , 3981 (2) , 0.77s , 1.000 , 0.82 ( 0.02 , 0.1383 ) \n",
      "2 , 5011 (1) , 8.89s , 1.000 , 0.79 ( 0.00 , 0.0000 ) \n",
      "2 , 6309 (1) , 5.64s , 1.000 , 0.80 ( 0.02 , 0.0535 ) \n",
      "2 , 7943 (1) , 1.66s , 1.000 , 0.79 ( 0.02 , 0.0000 ) \n",
      "2 , 10000 (1) , 3.86s , 1.000 , 0.79 ( 0.04 , 0.0066 ) \n",
      " \n",
      "idLogit, {'Lambda1': 0.1, 'Lambda2': 1.0}\n",
      "1 , 1000 (1) , 0.31s , 0.999 , 0.88 ( 0.01 , 0.2384 ) \n",
      "1 , 1258 (2) , 0.84s , 1.000 , 0.82 ( 0.28 , 0.0593 ) \n",
      "1 , 1584 (2) , 0.81s , 1.000 , 0.82 ( 0.06 , 0.0000 ) \n",
      "1 , 1995 (2) , 0.71s , 1.000 , 0.84 ( 0.05 , 0.0325 ) \n",
      "1 , 2511 (2) , 0.74s , 0.999 , 0.85 ( 0.05 , 0.2101 ) \n",
      "1 , 3162 (1) , 0.16s , 1.000 , 0.84 ( 0.05 , 0.3490 ) \n",
      "1 , 3981 (2) , 0.62s , 1.000 , 0.82 ( 0.21 , 0.1137 ) \n",
      "1 , 5011 (2) , 1.35s , 1.000 , 0.79 ( 0.10 , 0.0199 ) \n",
      "1 , 6309 (2) , 0.56s , 1.000 , 0.80 ( 0.12 , 0.1581 ) \n",
      "1 , 7943 (2) , 2.19s , 1.000 , 0.79 ( 0.11 , 0.0158 ) \n",
      "1 , 10000 (1) , 0.47s , 1.000 , 0.79 ( 0.12 , 0.4467 ) \n",
      " \n",
      "2 , 1000 (1) , 3.45s , 0.999 , 0.88 ( 0.01 , 0.3379 ) \n",
      "2 , 1258 (2) , 0.81s , 0.999 , 0.86 ( 0.31 , 0.0372 ) \n",
      "2 , 1584 (2) , 0.93s , 1.000 , 0.82 ( 0.22 , 0.3500 ) \n",
      "2 , 1995 (2) , 0.90s , 1.000 , 0.82 ( 0.18 , 0.3190 ) \n",
      "2 , 2511 (1) , 2.14s , 0.999 , 0.85 ( 0.03 , 0.4964 ) \n",
      "2 , 3162 (2) , 0.55s , 1.000 , 0.84 ( 0.06 , 0.3194 ) \n",
      "2 , 3981 (2) , 0.63s , 1.000 , 0.82 ( 0.21 , 0.1262 ) \n",
      "2 , 5011 (2) , 1.29s , 1.000 , 0.78 ( 0.12 , 0.1566 ) \n",
      "2 , 6309 (1) , 0.27s , 1.000 , 0.80 ( 0.12 , 0.1388 ) \n",
      "2 , 7943 (1) , 0.16s , 1.000 , 0.79 ( 0.13 , 0.1582 ) \n",
      "2 , 10000 (1) , 0.10s , 1.000 , 0.79 ( 0.13 , 0.6786 ) \n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "mdls = [\n",
    "    { 'code' : 'mnl' } , \n",
    "    { 'code' : 'lcl' , 'data' : { 'C' : 3 } } , \n",
    "    { 'code' : 'ccl' , 'data' : { 'C' : 3 } } , \n",
    "    { 'code' : 'saa' } , \n",
    "    { 'code' : 'glq' } , \n",
    "    { 'code' : 'idl' } , \n",
    "    { 'code' : 'idl' , 'data' : { 'Lambda1' : 10.0 , 'Lambda2' : 1.0 } } , \n",
    "    { 'code' : 'idl' , 'data' : { 'Lambda1' :  1.0 , 'Lambda2' : 1.0 } } , \n",
    "    { 'code' : 'idl' , 'data' : { 'Lambda1' :  0.1 , 'Lambda2' : 1.0 } } , \n",
    "]\n",
    "fitr = ModelFitExp( I=100 , dgp=mnl_dgp , timeout=60 , models=mdls , trials=2 , verbose=False )\n",
    "# fitr.set_obsnums( 3 , 6 , 25 )\n",
    "# fitr.set_obsnums( 3 , 5 , 20 )\n",
    "fitr.set_obsnums( 3 , 4 , 10 )\n",
    "\n",
    "fitr.run()\n",
    "\n",
    "fitr.print_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Class Logit data\n",
    "\n",
    "Let's try Latent Class Logit data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/morrowwr/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:62: RuntimeWarning: invalid value encountered in log\n",
      "/Users/morrowwr/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:62: RuntimeWarning: invalid value encountered in log\n",
      "/Users/morrowwr/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:62: RuntimeWarning: invalid value encountered in log\n",
      "/Users/morrowwr/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:62: RuntimeWarning: invalid value encountered in log\n",
      "/Users/morrowwr/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:62: RuntimeWarning: invalid value encountered in log\n",
      "/Users/morrowwr/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:62: RuntimeWarning: invalid value encountered in log\n",
      "/Users/morrowwr/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:62: RuntimeWarning: invalid value encountered in log\n",
      "/Users/morrowwr/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:62: RuntimeWarning: invalid value encountered in log\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mdls = [\n",
    "    { 'code' : 'mnl' } , \n",
    "    { 'code' : 'lcl' , 'data' : { 'C' : 3 } } , \n",
    "    { 'code' : 'ccl' , 'data' : { 'C' : 3 } } , \n",
    "    { 'code' : 'saa' } , \n",
    "    { 'code' : 'glq' } , \n",
    "    { 'code' : 'idl' } ,  \n",
    "    { 'code' : 'idl' , 'data' : { 'Lambda1' : 10.0 , 'Lambda2' : 0.0 } } , \n",
    "    { 'code' : 'idl' , 'data' : { 'Lambda1' :  1.0 , 'Lambda2' : 0.0 } } , \n",
    "    { 'code' : 'idl' , 'data' : { 'Lambda1' :  0.1 , 'Lambda2' : 0.0 } } , \n",
    "]\n",
    "\n",
    "dgp = lambda N,I,i : lcl_dgp( N , I , i , 5 )\n",
    "\n",
    "fitr = ModelFitExp( I=100 , dgp=dgp , timeout=60 , models=mdls , trials=1 , verbose=False )\n",
    "# fitr.set_obsnums( 3 , 6 , 25 )\n",
    "fitr.set_obsnums( 3 , 5 , 15 )\n",
    "\n",
    "fitr.run()\n",
    "fitr.print_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random (Gaussian) Coefficient Logit data\n",
    "\n",
    "Let's look at the Gaussian coefficient case now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "mdls = [\n",
    "    { 'code' : 'mnl' } , \n",
    "    #{ 'code' : 'lcl' , 'data' : { 'C' : 3 } } , \n",
    "    #{ 'code' : 'ccl' , 'data' : { 'C' : 3 } } , \n",
    "    #{ 'code' : 'saa' } , \n",
    "    #{ 'code' : 'glq' } , \n",
    "    { 'code' : 'idl' } ,  \n",
    "    { 'code' : 'idl' , 'data' : { 'Lambda1' : 10.0 , 'Lambda2' : 0.0 } } , \n",
    "    { 'code' : 'idl' , 'data' : { 'Lambda1' :  1.0 , 'Lambda2' : 0.0 } } , \n",
    "    { 'code' : 'idl' , 'data' : { 'Lambda1' :  0.1 , 'Lambda2' : 0.0 } } , \n",
    "]\n",
    "dgp = lambda N,I,i : grc_dgp(N,I,i,sigma=0.5)\n",
    "fitr = ModelFitExp( I=1000 , dgp=dgp , timeout=60 , models=mdls , trials=1 , verbose=True )\n",
    "# fitr.set_obsnums( 3 , 6 , 25 )\n",
    "fitr.set_obsnums( 3 , 5 , 15 )\n",
    "\n",
    "fitr.run()\n",
    "fitr.print_results()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
